{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "49401851",
   "metadata": {},
   "source": [
    "## Uncertainty Quantification"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3956e351",
   "metadata": {},
   "source": [
    "In traditional *supervised learning*, we have data features and target data. The result is to have a neural network that maps input data that maps to predicted target that maps the true targets.\n",
    "\n",
    "This gives us an expected target, but we want to go beyond this and learn the uncertainties associated with it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61e0d9b5",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/probs.png\" alt=\"probs\" style=\"width:30%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2641dfd",
   "metadata": {},
   "source": [
    "A big theme with this tutorial is: **Do not mistake probability for model confidence**. I will explain this shortly.\n",
    "\n",
    "Let's say we train a neural network to discriminate between cats and dogs based on images. This network learns features in these images, such as whiskers for cats or floppy ears for dogs. This networks creates an internal representation to discern features from these images and then makes a decision whether it is a cat or a dog. The last layer is `softmax` in order to create probabilities that sum to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e2d74c3",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/pet_model.png\" alt=\"probs\" style=\"width:35%; height:auto; object-fit:cover; object-position:top; margin-top:-4%; clip-path:inset(20% 0 0 0);\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90952208",
   "metadata": {},
   "source": [
    "The main challenge in uncertainty quantification and anomaly detection is that models are typically trained on clean, well-curated data. However, real-world data is often messy and unpredictable.\n",
    "\n",
    "When we deploy our models, it's not enough to simply classify an image—we also need to know how confident the model is in its prediction.\n",
    "\n",
    "For example, if we train a network to distinguish between cats and dogs, and then test it on an image of a boat (something it has never seen before), the model will still output a probability for \"cat\" or \"dog.\" However, it won't indicate how uncertain it is about this prediction.\n",
    "\n",
    "We need a way for a model to say **hmm...I don't know** <img src=\"images/hmm.png\" alt=\"hmm\" style=\"height:1em; vertical-align:middle;\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fff2e4c1",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"display: flex; justify-content: center; gap: 2em;\">\n",
    "    <img src=\"images/reality.png\" alt=\"reality\" style=\"width:35%; height:auto;\">\n",
    "    <img src=\"images/new.png\" alt=\"new\" style=\"width:35%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fb43edb",
   "metadata": {},
   "source": [
    "There are two main types of uncertainty:\n",
    " - **Aleatoric Uncertainty**\n",
    "    - Relates to the inherent noise or randomness in the input data\n",
    "    - Becomes significant when the data is noisy or ambiguous\n",
    "    - Cannot be minimized by collecting more data\n",
    " - **Epistemic Uncertainty**\n",
    "    - Reflects uncertainty in the model’s predictions due to limited knowledge\n",
    "    - Is high when the model has not seen enough diverse training examples\n",
    "    - Can be reduced by gathering additional data or improving the model\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/uncertainty.png\" alt=\"probs\" style=\"width:30%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed014925",
   "metadata": {},
   "source": [
    "Aleatoric uncertainty can be learned directly using neural networks, but epistemic uncertainty is much more challenging to estimate. \n",
    "\n",
    "Q: *How can a model understand when it doesn't know the answer?*\n",
    "\n",
    "A: Instead of learning a fixed set of weight numbers, we replace those numbers with distributions. This is called Bayesian neural networks. However, this problem becomes intractable, so we use approximations."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26918600",
   "metadata": {},
   "source": [
    "These approximations involve running $T$ forward passes on the same input data using different weights. For example:\n",
    " - **Dropout** (left image) randomly zeros out some number of nodes, and the spread of those predictions gives you an estimate of the epistemic uncertainty.\n",
    " - **Ensemble** (right image) involves independently training $T$ different models, and the variance of that gives you the epistemic uncertainty.\n",
    "\n",
    "<div align=\"center\" style=\"display: flex; justify-content: center; gap: 2em;\">\n",
    "    <img src=\"images/dropout.png\" alt=\"Dropout (left)\" style=\"width:25%; height:auto;\">\n",
    "    <img src=\"images/ens.png\" alt=\"Ensemble (right)\" style=\"width:25%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec2bc269",
   "metadata": {},
   "source": [
    "There are downsides to Bayesian Deep Learning, which include:\n",
    " - **Slow**: Requires running the network $T$ times for each input\n",
    " - **Memory**: Stores $T$ copies of the network in parallel\n",
    " - **Efficiency**: Sampling hinders real-time on edge devices\n",
    " - **Calibration**: Sensitive to prior and often over-confident"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42e3ff5f",
   "metadata": {},
   "source": [
    "Let's say that each model has a mean and a data uncertainty, like the top left image. If you have an ensemble of these models (bottom left image), We can see that each of these ensemble puts an X on the rightmost graph. The x-axis is the mean $\\mu$ while the y-axis is the data uncertainty $\\sigma^2$. \n",
    "\n",
    "Instead of this brute force sampling, can we instead **directly** learn the parameters defining this underlying likelihood distribution?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1528fb6",
   "metadata": {},
   "source": [
    "<div style=\"max-width:700px; margin:auto;\">\n",
    "    <div style=\"display: flex; align-items: flex-start; gap: 2em;\">\n",
    "            <div style=\"display: flex; flex-direction: column; justify-content: space-between; height: 300px;\">\n",
    "                    <img src=\"images/im1.png\" alt=\"im1\" style=\"max-height:145px; \">\n",
    "                    <img src=\"images/im2.png\" alt=\"im2\" style=\"max-height:145px; margin-top:2%; object-fit:contain; aspect-ratio:1/1;\">\n",
    "            </div>\n",
    "            <img src=\"images/im3.png\" alt=\"im3\" style=\"max-height:310px; object-fit:contain;\">\n",
    "    </div>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b69c7de4",
   "metadata": {},
   "source": [
    "## Evidential Deep Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b7fc1fb",
   "metadata": {},
   "source": [
    "Evidential Deep Learning (EDL) recasts the learning as an evidence acquisition process. The more evidence we have, the more confidence we have. It takes a *Theory of Evidence* perspective: $\\operatorname{softmax}$ is interpreted as the parameter set of a categorical distribution which is replaced by the parameters of a **Dirichlet density** (a factory of softmax point estimates). From left to right, we have:\n",
    " - Low uncertainties,, which indicates high confidence\n",
    " - High aleatoric (data) uncertainty\n",
    " - High epistemic (model) uncertainty\n",
    "\n",
    " **Goal: train a neural network to learn these type of evidential distributions**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4733eaaa",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/dists.png\" alt=\"dists\" style=\"width:50%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3be43255",
   "metadata": {},
   "source": [
    "In this tutorial, we focus on classification, though it is also applicable to regression. More information on EDL for regression is available [here](https://arxiv.org/abs/1910.02600).\n",
    "\n",
    "**Sampling from an evidential distribution yields individual new distributions over the data.** So you can think of this as a second-order learning problem. You want to learn the $\\alpha_k$ parameters of a Dirichlet distribution ($\\alpha_k > 0 $). In the image below, we can see an example of a [Dirichlet distribution](https://en.wikipedia.org/wiki/Dirichlet_distribution) with three classes, making a triangle. More classes creates a [simplex](https://en.wikipedia.org/wiki/Simplex). These model parameters determine the density inside of this classification simplex. We replace the Categorical distribution that we've been using to get probabilities with a Dirichlet distribution. Sampling from this gives us our probabilities. For exampling, the middle of the Dirichlet distribution will result in equal probabilities for all classes. \n",
    "\n",
    "We choose the Dirichlet distribution as our *evidential* distribution because it is the conjugate prior in the context of Bayesian inference. The proofs for why these are chosen are available in the original [paper](https://papers.nips.cc/paper_files/paper/2018/hash/a981f2b708044d6fb4a71a1463242520-Abstract.html).\n",
    "\n",
    "<div align=\"center\">\n",
    "    <img src=\"images/dirichlet.png\" alt=\"dists\" style=\"width:30%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22c15828",
   "metadata": {},
   "source": [
    "For our network with model weights $\\Theta$, instead of outputting the probabilities using a `softmax` layer, we use a `ReLU` layer to ensure $\\alpha_k > 0$. Then, we perform multi-objective training by modifying the loss function, shown in the images below. We include a reconstruction loss to ensure the accuracy is high, and we add a regularization term to penalize large confidence to uncertain samples. You achieve this through KL divergence between you predicted Dirichlet distribution and the unit Dirichlet distribution ($\\alpha_k=1$) which is completely uncertain. The $\\lambda_t$ term controls the strength of the regularization term. Similar to the original paper, we increase $\\lambda_t$ over epochs.\n",
    "$\n",
    "\\begin{align}\n",
    "    \\mathcal{L}(\\Theta)=\\sum_{i=1}^{N}\\mathcal{L}_{MSE}(\\Theta)_i+\\lambda_t\\sum_{i=1}^{N}\\mathcal{L}_{KL}(\\Theta)_i\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4694244",
   "metadata": {},
   "source": [
    "<div align=\"center\" style=\"display: flex; justify-content: center; gap: 2em;\">\n",
    "    <img src=\"images/model.png\" alt=\"Model (left)\" style=\"width:25%; height:auto;\">\n",
    "    <img src=\"images/diagram.png\" alt=\"Diagram (right)\" style=\"width:25%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5733aea9",
   "metadata": {},
   "source": [
    "Specifically, the outputs of the network, denoted as $f_k(\\mathbf{x}|\\Theta)$, directly provide the evidence for the anticipated Dirichlet distribution through.\n",
    "$$\n",
    "\\begin{equation*}\n",
    "e_k = f_k(\\mathbf{x}|\\Theta) ~~~\\textrm{and}~~~ \\alpha_k = f_k(\\mathbf{x}|\\Theta) + 1\n",
    "\\end{equation*}\n",
    "$$\n",
    "\n",
    "Once the network learns the parameters $\\alpha$, its mean, can be taken as an estimate of the class probabilities: $$p_k = \\mathbb{E}(x_k) = \\frac{\\alpha_k}{S}$$ where the sum $S = \\sum_{k=1}^{K}\\alpha_k = \\sum_{k=1}^{K}(e_k + 1)$ represents the Dirichlet strength. The epistemic uncertainty can then be estimated as $$u = \\frac{K}{S}$$ where $K$ is the number of classes. More information is available in our paper [here](https://iopscience.iop.org/article/10.1088/2632-2153/ade51b).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eeafcd1a",
   "metadata": {},
   "source": [
    "## Getting started"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00e8c8e2",
   "metadata": {},
   "source": [
    "If you're running within Google Colab:\n",
    "- Go to Runtime -> Change runtime type and select GPU\n",
    "\n",
    "If you're running locally  \n",
    "- Run `pip install -r requirements.txt` to install the necessary requirements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d24c077",
   "metadata": {},
   "source": [
    "#### Import all necessary packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0225b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!rm -rf GDSVirtualTutorials\n",
    "!git clone https://github.com/butler-julie/GDSVirtualTutorials.git\n",
    "%cd GDSVirtualTutorials/100325_UncertaintyQuantification/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11b12a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec7c428f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import os, json, sys\n",
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "import seaborn as sns\n",
    "import h5py\n",
    "from matplotlib.colors import LogNorm\n",
    "from EvalTools import *\n",
    "import gc\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a135ac7e",
   "metadata": {},
   "source": [
    "#### Downloading and preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c53032e",
   "metadata": {},
   "source": [
    "We apply EDL to identify jets at LHC using an explainable AI inspired $t$-quark tagger called the Particle Flow Interaction Network (PFIN). The first dataset (`TopData`) has 2M simulated jets and two classes: top quark jets (label=1) and QCD jets (label=0)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65906e9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "A subdirectory or file processed already exists.\n",
      "Error occurred while processing: processed.\n",
      "'wget' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'unzip' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'mv' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "'rm' is not recognized as an internal or external command,\n",
      "operable program or batch file.\n",
      "c:\\Users\\khota\\miniconda3\\envs\\robot\\python.exe: can't open file 'c:\\\\Users\\\\khota\\\\GDSVirtualTutorials\\\\100325_UncertaintyQuantification\\\\datasets\\\\topdata\\\\topdata_prepocess.py': [Errno 2] No such file or directory\n"
     ]
    }
   ],
   "source": [
    "orig_dir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    os.chdir('datasets/topdata')\n",
    "    !rm -rf processed\n",
    "    !rm download\n",
    "    !rm -rf raw\n",
    "    !mkdir -p processed\n",
    "    !wget https://desycloud.desy.de/index.php/s/llbX3zpLhazgPJ6/download\n",
    "    !unzip download\n",
    "    !mv v0/ raw/\n",
    "    !rm download\n",
    "    !python topdata_preprocess.py\n",
    "finally:\n",
    "    os.chdir(orig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b49cb57",
   "metadata": {},
   "source": [
    "<div align=\"center\">\n",
    "    <img src=\"images/top.png\" alt=\"top\" style=\"width:50%; height:auto;\">\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "525f9ac5",
   "metadata": {},
   "source": [
    "We also use another dataset called `JetNet`. This dataset consists of 880k particle jets originating from gluons ($g$), light quarks ($q$), top quarks ($t$), and bosons ($W$ and $Z$)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0e6355a",
   "metadata": {},
   "outputs": [],
   "source": [
    "orig_dir = os.getcwd()\n",
    "\n",
    "try:\n",
    "    os.chdir('datasets/jetnet')\n",
    "    !mkdir -p raw\n",
    "    !mkdir -p processed\n",
    "    !wget -O raw/g.hdf5 \"https://zenodo.org/records/6975118/files/g.hdf5?download=1\"\n",
    "    !wget -O raw/q.hdf5 \"https://zenodo.org/records/6975118/files/q.hdf5?download=1\"\n",
    "    !wget -O raw/t.hdf5 \"https://zenodo.org/records/6975118/files/t.hdf5?download=1\"\n",
    "    !wget -O raw/w.hdf5 \"https://zenodo.org/records/6975118/files/w.hdf5?download=1\"\n",
    "    !wget -O raw/z.hdf5 \"https://zenodo.org/records/6975118/files/z.hdf5?download=1\"\n",
    "    !python jetnet_preprocess.py\n",
    "finally:\n",
    "    os.chdir(orig_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd3f2065",
   "metadata": {},
   "source": [
    "We specify the reconstruction loss below as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_{MSE}(\\Theta)_i \n",
    "    &=\n",
    "    \\sum_{k=1}^K\\mathbb{E} [ \\left(y_{ik} - { \\hat{y}}_{ik}\\right)^2 ] { \\,\\,= \\sum_{k=1}^K\\mathbb{E} \\left[ \\left(y_{ik} - \\frac{f_i(x_{k}|\\Theta) + 1}{\\sum_{j=1}^{K}(f_j(x_{k}|\\Theta) + 1)}\\right)^2 \\right]} \n",
    "\\end{align}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03a12efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def LossMSE(labels, outs):\n",
    "    # labels size: (Nb, nclasses) [true values]\n",
    "    # outs size: (Nb, nclasses) [NN predictions]\n",
    "    alphas = outs + 1\n",
    "    S = torch.sum(alphas, 1).reshape(-1,1)\n",
    "    probs = alphas / S\n",
    "    return ((labels - probs)**2 + probs * (1 - probs) / (1 + S)).sum(1).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c203368",
   "metadata": {},
   "source": [
    "The second component of the loss function is a KL Divergence term defined as\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "    \\mathcal{L}_{KL}(\\Theta)_i&=KL[D(\\mathbf{{\\hat{y}}}_i|\\mathbf{\\tilde{\\alpha}_i})\\|D(\\mathbf{{ \\hat{y}}}_i|\\left \\langle 1,\\cdots,1 \\right \\rangle)] \\\\\n",
    "    &=\\log \\left (\\frac{\\Gamma(\\sum_{k=1}^K\\tilde{\\alpha}_{ik})}{\\Gamma(K)\\prod_{k=1}^K\\Gamma(\\tilde{\\alpha}_{ik})} \\right ) + \\sum_{k=1}^K (\\tilde{\\alpha}_{ik}-1) \\left [ \\psi(\\tilde{\\alpha}_{ik}) - \\psi\\left(\\sum_{j=1}^K \\tilde{\\alpha}_{ij}\\right) \\right] \\nonumber\n",
    "\\end{align}\n",
    "$$\n",
    "where \n",
    "$$\n",
    "\\begin{equation*}\n",
    "    \\mathbf{\\tilde{\\alpha}_i} = \\mathbf{y_i} + (1 - \\mathbf{y_i}) \\odot \\mathbf{\\alpha_i} { ~~~\\textrm{and}~~~ \\alpha_i = f_i(\\mathbf{x}|\\Theta) + 1}\n",
    "\\end{equation*}\n",
    "$$\n",
    "and $\\psi(\\cdot)$ is the digamma function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e1dbdad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def KLDiv(labels, outs):\n",
    "    K = torch.tensor(labels.shape[-1]).float()\n",
    "    alphas = outs + 1\n",
    "    _alphas = labels + (1-labels)*alphas\n",
    "    _S = torch.sum(_alphas, 1).reshape(-1,1)\n",
    "    lognum = torch.lgamma(_S)\n",
    "    logden = torch.lgamma(K*1.0) + torch.lgamma(_alphas).sum(1).reshape(-1,1)\n",
    "    t2 = ((_alphas - 1) * (torch.digamma(_alphas) - torch.digamma(_S) )).sum(1).reshape(-1,1)\n",
    "    return (lognum - logden + t2).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44eadf5f",
   "metadata": {},
   "source": [
    "## `TopData` Model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40bcbc8a",
   "metadata": {},
   "source": [
    "### Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "929b1eb6",
   "metadata": {},
   "source": [
    "First, we load the pre-loaded parameters for our model. The name of the file corresponds to the $\\lambda_t$ used in the loss. `nominal` means increasing over epochs with slope 0.1. `1.0` refers to the maximum $\\lambda_t$. You can change the name of your file to use different strengths for the KL Divergence term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7315378c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'outdir': './trained_models/',\n",
       " 'outdictdir': './trained_model_dicts/',\n",
       " 'Np': 60,\n",
       " 'n_phiI': 128,\n",
       " 'x_mode': 'sum',\n",
       " 'phi_nodes': '100,100,64',\n",
       " 'f_nodes': '64,100,100',\n",
       " 'epochs': 50,\n",
       " 'label': 'topdata_nominal_1.0_baseline',\n",
       " 'batch_size': 250,\n",
       " 'data_loc': '../datasets/',\n",
       " 'data_type': 'topdata',\n",
       " 'preload': False,\n",
       " 'preload_file': '',\n",
       " 'klcoef': 'nominal',\n",
       " 'massrange': 'AND:0,10000.',\n",
       " 'ptrange': 'AND:0,10000',\n",
       " 'etarange': 'AND:-6,6',\n",
       " 'skiplabels': '',\n",
       " 'batchmode': True,\n",
       " 'use_softmax': False,\n",
       " 'use_dropout': False,\n",
       " 'ndata': 0}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_name = 'UQPFIN_topdata_nominal_1.0_baseline.json'\n",
    "parameters = json.load(open(f'json_files/{file_name}'))\n",
    "parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce826cc3",
   "metadata": {},
   "source": [
    "Then, we can train the model and save the checkpoints based on the highest accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "21493ed9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"c:\\Users\\khota\\GDSVirtualTutorials\\100325_UncertaintyQuantification\\train.py\", line 115, in <module>\n",
      "    with open(args.load_json, 'rt') as f:\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "FileNotFoundError: [Errno 2] No such file or directory: 'UQPFIN_topdata_nominal_1.0_baseline.json'\n"
     ]
    }
   ],
   "source": [
    "!python train.py --load-json json_files/UQPFIN_topdata_nominal_1.0_baseline.json"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a16adf",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ee60f7",
   "metadata": {},
   "source": [
    "We can evaluate the results on a test set and visualize the uncertainties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac37a9fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python evaluate_model.py --data topdata --make-file --type edl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14cab268",
   "metadata": {},
   "outputs": [],
   "source": [
    "#optional parameters to only evaluate certain models\n",
    "optional_dataset = 'topdata'\n",
    "optional_tag = ''\n",
    "results_dir = 'results/'\n",
    "saved_model_loc = \"./trained_models/\"\n",
    "saved_model_dict_loc = \"./trained_model_dicts/\"\n",
    "\n",
    "result_files = sorted([f for f in os.listdir(results_dir) if optional_dataset in f and optional_tag in f\n",
    "                      and \"Ensemble\" not in f and \"MCDO\" not in f and '.h5' in f and 'slope' not in f and 'mask' not in f])\n",
    "print('\\n'.join(result_files))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52ca66b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelname in result_files:\n",
    "    model_results = {}\n",
    "    mname = modelname[20:-3]\n",
    "    filename = os.path.join(results_dir, modelname)\n",
    "    \n",
    "    f = h5py.File(filename, \"r\")\n",
    "    labels, preds, oods, probs, uncs, sums = f['labels'][:], f['preds'][:], f['oods'][:], f['probs'][:], f['uncs'][:], f['sums'][:]\n",
    "    f.close()\n",
    "    acc = accuracy_score(labels[~oods], preds[~oods])*100\n",
    "    if \"topdata\" in mname:\n",
    "        probs2=probs\n",
    "    else:\n",
    "        skiplabels = np.unique(labels[oods])\n",
    "        probs2=np.delete(probs, skiplabels, 1)\n",
    "\n",
    "    if probs2.shape[1] == 2:\n",
    "        probs2 = probs2[:, 1]\n",
    "    \n",
    "    # misclassification detection vs out-of-distribution detection\n",
    "    if \"baseline\" in mname:\n",
    "        oods = labels != preds\n",
    "    \n",
    "    this_file = os.path.join(saved_model_loc, modelname[8:-3])\n",
    "    evaluator = ModelEvaluator(this_file)\n",
    "    nparams = sum(p.numel() for p in evaluator.model.parameters())\n",
    "    del evaluator\n",
    "    gc.collect()\n",
    "    \n",
    "    # Sensory et al EDL Uncertainty\n",
    "    auc = roc_auc_score(oods, uncs) * 100\n",
    "    \n",
    "    # D-STD Uncertainty\n",
    "    sums = torch.from_numpy(sums).reshape(-1,1)\n",
    "    probs = torch.from_numpy(probs)\n",
    "    uncs = torch.sqrt(((probs*(1 - probs))/(sums + 1))).sum(1).numpy()\n",
    "    uncs = torch.sqrt(((probs*(1 - probs))/(sums + 1)).sum(1)).numpy()\n",
    "    auc_std = roc_auc_score(oods, uncs) * 100\n",
    "\n",
    "    print(\"{} \\t\\t Params: {}\\t Accuracy: {:.2f}% \\t AUROC: {:.2f}% \\t AUROC-STD: {:.2f}%\".format(mname, nparams, acc, auc, auc_std))    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e724260",
   "metadata": {},
   "outputs": [],
   "source": [
    "for modelname in result_files:\n",
    "    key = modelname[20:-3] \n",
    "    print(key)\n",
    "    \n",
    "    if \"jetnet\" in key:\n",
    "        l_max = 5\n",
    "        fsize=18\n",
    "        groups = ['QCD', 'QCD', 'Top', 'Bosons', 'Bosons']\n",
    "    elif \"jetclass\" in key:\n",
    "        l_max = 10\n",
    "        fsize=10\n",
    "        groups = ['QCD', 'Higgs', 'Higgs', 'Higgs', 'Higgs', 'Higgs', 'Bosons', 'Bosons', 'Top', 'Top']\n",
    "    elif \"JNqgmerged\" in key:\n",
    "        l_max = 4\n",
    "    else:\n",
    "        l_max = 2\n",
    "        fsize=24\n",
    "        groups = [\"background\", \"signal\"]\n",
    "    groups = np.array(groups)\n",
    "        \n",
    "    uniq_names, ind = np.unique(groups, return_index=True)\n",
    "    uniq_names = uniq_names[np.argsort(ind)]\n",
    "        \n",
    "        \n",
    "    filename = os.path.join(results_dir, modelname)\n",
    "    \n",
    "    f = h5py.File(filename, \"r\")    \n",
    "    labels, preds, maxprobs, sums, oods, uncs, probs = f['labels'][:], f['preds'][:], \\\n",
    "                                                f['maxprobs'][:], f['sums'][:], \\\n",
    "                                                f['oods'][:], f['uncs'][:], f['probs'][:]\n",
    "    f.close()\n",
    "    \n",
    "    split = key.split('_')\n",
    "    coeff = \"_\".join(split[1:-1])\n",
    "    # Choose to use D-STD Uncertainty or not\n",
    "    for use_std in [False]:\n",
    "        high_unc = 0.8\n",
    "        if use_std:\n",
    "            sums = torch.from_numpy(sums).reshape(-1,1)\n",
    "            probs = torch.from_numpy(probs)\n",
    "            uncs = torch.sqrt(((probs*(1 - probs))/(sums + 1))).sum(1).numpy()\n",
    "            coeff += \"_std\"\n",
    "            high_unc = 0.5\n",
    "        \n",
    "        savefolder = \"figures/{}/{}/{}/\".format(split[0], split[-1], coeff)\n",
    "        os.makedirs(savefolder, exist_ok=True)\n",
    "        \n",
    "        if 'baseline' in key:\n",
    "            f1 = 'correct'\n",
    "            f2 = 'incorrect'\n",
    "            oods = labels != preds\n",
    "        else:\n",
    "            f1 = 'id'\n",
    "            f2 = 'ood'\n",
    "        \n",
    "        \n",
    "        mult_label = \"Unc\"\n",
    "        if \"_0_\" in key:\n",
    "            if \"topdata\" in key:\n",
    "                multiple = 300\n",
    "                if use_std:\n",
    "                    multiple = 30\n",
    "            elif \"jetnet\" in key:\n",
    "                if \"skiptop\" in key:\n",
    "                    multiple = 5\n",
    "                elif \"skiptwz\" in key:\n",
    "                    multiple = 25\n",
    "                else:\n",
    "                    multiple = 5*2.5\n",
    "                    if \"skipwz\" in key and not use_std:\n",
    "                        multiple *= 4\n",
    "            elif \"jetclass\" in key:\n",
    "                if \"baseline\" in key:\n",
    "                    multiple = 15\n",
    "                elif \"skipwz\" in key:\n",
    "                    multiple = 5*2.5\n",
    "                else:\n",
    "                    multiple = 25\n",
    "            uncs = uncs * multiple\n",
    "            mult_label = str(multiple) + r' $\\times$ Unc'\n",
    "        \n",
    "        for types in ['total', 'separate', 'total_log', 'separate_log']:\n",
    "            # Uncertainty distribution\n",
    "            plt.figure(figsize=(10,10))\n",
    "            ax = plt.gca()\n",
    "                \n",
    "            if 'total' in types:\n",
    "                ax.hist(uncs, bins=np.arange(0.,1.01,0.04), alpha = 0.7, color='blue', weights = (1./len(uncs)) * np.ones_like(uncs))\n",
    "            else:\n",
    "                ax.hist(uncs[~oods], bins=np.arange(0.,1.01,0.04), label=f1, alpha = 0.5, histtype = 'step', linewidth = 5, weights = (1./len(uncs[~oods])) * np.ones_like(uncs[~oods]))\n",
    "                ax.hist(uncs[oods], bins=np.arange(0.,1.01,0.04), label=f2, alpha = 0.5, histtype = 'step', linewidth = 5, weights = (1./len(uncs[oods])) * np.ones_like(uncs[oods]))\n",
    "                ax.legend(fontsize = 22)\n",
    "            # ax.set_aspect((ax.get_xlim()[1] - ax.get_xlim()[0]) / (ax.get_ylim()[1] - ax.get_ylim()[0]))\n",
    "            ax.set_xlabel(\"Uncertainty\".replace(\"Unc\", mult_label), fontsize=30)\n",
    "            ax.set_ylabel(\"Fractional Number of Events\", fontsize = 24)\n",
    "            if 'log' in types:\n",
    "                ax.set_ylabel(\"Log10(Fractional Number of Events)\", fontsize = 24)\n",
    "                ax.set_yscale('log')\n",
    "            ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "            ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(\"{}/unc_normal_{}.pdf\".format(savefolder, types), dpi = 150, bbox_inches='tight')\n",
    "            \n",
    "            \n",
    "        for types in ['total', 'separate', 'total_log', 'separate_log']:\n",
    "            # Uncertainty distribution\n",
    "            plt.figure(figsize=(10,10))\n",
    "            ax = plt.gca()\n",
    "                \n",
    "            if 'total' in types:\n",
    "                ax.hist(uncs, bins=np.arange(0.,1.01,0.04), alpha = 0.7, color='blue', weights = (1./len(uncs)) * np.ones_like(uncs))\n",
    "            else:\n",
    "                ax.hist(uncs[~oods], bins=np.arange(0.,1.01,0.04), label=f1, alpha = 0.5, histtype = 'step', linewidth = 5)\n",
    "                ax.hist(uncs[oods], bins=np.arange(0.,1.01,0.04), label=f2, alpha = 0.5, histtype = 'step', linewidth = 5)\n",
    "                ax.legend(fontsize = 22)\n",
    "            # ax.set_aspect((ax.get_xlim()[1] - ax.get_xlim()[0]) / (ax.get_ylim()[1] - ax.get_ylim()[0]))\n",
    "            ax.set_xlabel(\"Uncertainty\".replace(\"Unc\", mult_label), fontsize=30)\n",
    "            ax.set_ylabel(\"Number of Events\", fontsize = 24)\n",
    "            if 'log' in types:\n",
    "                ax.set_ylabel(\"Log10(Number of Events)\", fontsize = 24)\n",
    "                ax.set_yscale('log')\n",
    "            ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "            ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(\"{}/unc_{}.pdf\".format(savefolder, types), dpi = 150, bbox_inches='tight')\n",
    "            \n",
    "        for types in ['normal', 'log']:\n",
    "            # Uncertainty distribution\n",
    "            plt.figure(figsize=(10,10))\n",
    "            ax = plt.gca()\n",
    "            \n",
    "            for i in range(len(uniq_names)):\n",
    "                indices = np.in1d(labels, np.where(groups == uniq_names[i])[0])\n",
    "                if np.any(indices & oods) and \"skip\" in key:\n",
    "                    lstyle = '--'\n",
    "                    lbel = f\"{uniq_names[i]} (OOD)\"\n",
    "                else:\n",
    "                    lstyle = '-'\n",
    "                    lbel = uniq_names[i]\n",
    "                ax.hist(uncs[indices], bins=np.arange(0.,1.01,0.04), label=lbel, alpha = 0.5, histtype = 'step', linewidth = 5, linestyle = lstyle, weights = (1./len(uncs[indices])) * np.ones_like(uncs[indices]))\n",
    "            ax.legend(fontsize = 22)\n",
    "            # ax.set_aspect((ax.get_xlim()[1] - ax.get_xlim()[0]) / (ax.get_ylim()[1] - ax.get_ylim()[0]))\n",
    "            ax.set_xlabel(\"Uncertainty\".replace(\"Unc\", mult_label), fontsize=30)\n",
    "            ax.set_ylabel(\"Fractional Number of Events\", fontsize = 24)\n",
    "            if 'log' in types:\n",
    "                ax.set_ylabel(\"Log10(Fractional Number of Events)\", fontsize = 24)\n",
    "                ax.set_yscale('log')\n",
    "            ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "            ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(\"{}/unc_class_{}.pdf\".format(savefolder, types), dpi = 150, bbox_inches='tight')\n",
    "            \n",
    "        if \"baseline\" in key:\n",
    "            for types in ['normal', 'log']:\n",
    "                # Uncertainty distribution\n",
    "                plt.figure(figsize=(10,10))\n",
    "                ax = plt.gca()\n",
    "\n",
    "                for i in range(len(uniq_names)):\n",
    "                    indices = np.in1d(labels, np.where(groups == uniq_names[i])[0])\n",
    "                    if np.any(indices & oods) and \"skip\" in key:\n",
    "                        lstyle = '--'\n",
    "                        lbel = f\"{uniq_names[i]} (OOD)\"\n",
    "                    else:\n",
    "                        lstyle = '-'\n",
    "                        lbel = uniq_names[i]\n",
    "                    ax.hist(uncs[(labels == preds) & indices], bins=np.arange(0.,1.01,0.04), label=lbel, alpha = 0.5, histtype = 'step', linewidth = 5, linestyle = lstyle, weights = (1./len(uncs[(labels == preds) & indices])) * np.ones_like(uncs[(labels == preds) & indices]))\n",
    "                ax.legend(fontsize = 22)\n",
    "                # ax.set_aspect((ax.get_xlim()[1] - ax.get_xlim()[0]) / (ax.get_ylim()[1] - ax.get_ylim()[0]))\n",
    "                ax.set_xlabel(\"Uncertainty\".replace(\"Unc\", mult_label), fontsize=30)\n",
    "                ax.set_ylabel(\"Fractional Number of Events\", fontsize = 24)\n",
    "                if 'log' in types:\n",
    "                    ax.set_ylabel(\"Log10(Fractional Number of Events)\", fontsize = 24)\n",
    "                    ax.set_yscale('log')\n",
    "                ax.tick_params(axis='both', which='major', labelsize=24)\n",
    "                ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "                plt.tight_layout()\n",
    "                # plt.savefig(\"{}/unc_correct_class_{}.pdf\".format(savefolder, types), dpi = 150, bbox_inches='tight')\n",
    "            \n",
    "        \n",
    "        filetypes = ['total', f1, f2]\n",
    "        indices = [oods | ~oods, ~oods, oods]\n",
    "        \n",
    "        for filetype, idx in zip(filetypes, indices):\n",
    "            # Max Prob. vs Uncertainty distribution\n",
    "            hist, _, _ = np.histogram2d(maxprobs[idx], uncs[idx], bins = [np.arange(0.,1.01,0.04), np.arange(0.,1.01,0.04)])\n",
    "            \n",
    "            fig, ax  = plt.subplots(1, 2, figsize=(10,10), gridspec_kw={'width_ratios':[1,0.05], 'wspace': 0.1})\n",
    "            heatmap = sns.heatmap(hist.T, annot=False, cmap='winter', ax=ax[0], cbar_ax=ax[1])\n",
    "            ax[0].invert_yaxis()\n",
    "            ax[0].set_xlabel(\"Max. Prob.\", fontsize=30)\n",
    "            ax[0].set_ylabel(\"Uncertainty\".replace(\"Unc\", mult_label), fontsize=30)\n",
    "            ax[0].set_yticks(np.arange(0, 26, 5))\n",
    "            ax[0].set_yticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0], rotation=0)\n",
    "            ax[0].set_xticks(np.arange(0, 26, 5))\n",
    "            ax[0].set_xticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "            ax[0].tick_params(axis='both', which='major', labelsize=20)\n",
    "            cbar = heatmap.collections[0].colorbar\n",
    "            cbar.ax.tick_params(labelsize=24)\n",
    "            cbar.ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "            if 'topdata' in key or 'jetnet' in key:\n",
    "                tick_values = cbar.get_ticks()\n",
    "                cbar.set_ticklabels([f'{int(tick / 1000)}k' for tick in tick_values])\n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(\"{}/unc_prob_{}.pdf\".format(savefolder, filetype), dpi = 150, bbox_inches='tight')\n",
    "        \n",
    "        for filetype, idx in zip(filetypes, indices):\n",
    "            # Max Prob. vs Uncertainty distribution (Log Scale)\n",
    "            hist, _, _ = np.histogram2d(maxprobs[idx], uncs[idx], bins = [np.arange(0.,1.01,0.04), np.arange(0.,1.01,0.04)])\n",
    "            \n",
    "            fig, ax  = plt.subplots(1, 2, figsize=(10,10), gridspec_kw={'width_ratios':[1,0.05], 'wspace': 0.1})\n",
    "            heatmap = sns.heatmap(hist.T+1, annot=False, cmap='winter', ax=ax[0], cbar_ax=ax[1], norm=LogNorm())\n",
    "            ax[0].invert_yaxis()\n",
    "            ax[0].set_xlabel(\"Max. Prob.\", fontsize=30)\n",
    "            ax[0].set_ylabel(\"Uncertainty\".replace(\"Unc\", mult_label), fontsize=30)\n",
    "            ax[0].set_yticks(np.arange(0, 26, 5))\n",
    "            ax[0].set_yticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0], rotation=0)\n",
    "            ax[0].set_xticks(np.arange(0, 26, 5))\n",
    "            ax[0].set_xticklabels([0, 0.2, 0.4, 0.6, 0.8, 1.0])\n",
    "            ax[0].tick_params(axis='both', which='major', labelsize=20)\n",
    "            cbar = heatmap.collections[0].colorbar\n",
    "            cbar.ax.tick_params(labelsize=24)\n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(\"{}/unc_prob_{}_log.pdf\".format(savefolder, filetype), dpi = 150, bbox_inches='tight')\n",
    "   \n",
    "        for filetype, idx in zip(filetypes, indices):\n",
    "            # Labels vs Preds\n",
    "            arr = np.zeros((l_max,l_max))\n",
    "            for i in range(l_max):\n",
    "                for j in range(l_max):\n",
    "                    arr[j, i] = np.sum((labels[idx] == i) & (preds[idx] == j))\n",
    "            \n",
    "            fig, ax  = plt.subplots(1, 2, figsize=(10,10), gridspec_kw={'width_ratios':[1,0.05], 'wspace': 0.1})\n",
    "            heatmap = sns.heatmap(arr, annot=True, cmap='winter', annot_kws={\"size\": fsize, \"weight\": \"bold\"}, ax=ax[0], cbar_ax=ax[1])\n",
    "            ax[0].invert_yaxis()\n",
    "            ax[0].set_xlabel(\"Labels\", fontsize=30)\n",
    "            ax[0].set_ylabel(\"Preds\", fontsize=30)\n",
    "            ax[0].tick_params(axis='both', which='major', labelsize=24)\n",
    "            ax[0].tick_params(axis='y', labelrotation=0)\n",
    "            cbar = heatmap.collections[0].colorbar\n",
    "            cbar.ax.tick_params(labelsize=24)\n",
    "            cbar.ax.yaxis.get_offset_text().set_fontsize(24)\n",
    "            if 'topdata' in key or 'jetnet' in key:\n",
    "                tick_values = cbar.get_ticks()\n",
    "                cbar.set_ticklabels([f'{int(tick / 1000)}k' for tick in tick_values])\n",
    "            plt.tight_layout()\n",
    "            # plt.savefig(\"{}/labels_preds_{}.pdf\".format(savefolder, filetype), dpi = 150, bbox_inches='tight')\n",
    "            \n",
    "        for types in ['normal', 'log']:\n",
    "            for filetype, idx in zip(filetypes, indices):\n",
    "                # Labels vs Preds + Uncertainty\n",
    "                uncertainty_bins = np.zeros((5*l_max, l_max))  # 5 bins for uncertainty\n",
    "                for i in range(l_max):\n",
    "                    for j in range(l_max):\n",
    "                        filtered_indices = (labels[idx] == i) & (preds[idx] == j)\n",
    "                        if np.sum(filtered_indices) > 0:\n",
    "                            # Create histogram for uncertainty values in 5 bins\n",
    "                            hist, _ = np.histogram(uncs[idx][filtered_indices], bins=5, range=(0, 1))\n",
    "                            for k in range(5):\n",
    "                                uncertainty_bins[5*j+k, i] = hist[k]\n",
    "\n",
    "                fig, ax  = plt.subplots(1, 2, figsize=(10,10), gridspec_kw={'width_ratios':[1,0.05], 'wspace': 0.1})\n",
    "                if 'log' in types:\n",
    "                    norm = LogNorm()\n",
    "                else:\n",
    "                    norm = None\n",
    "                uncertainty_bins_clean = np.where(uncertainty_bins == 0, 1, uncertainty_bins)\n",
    "                heatmap = sns.heatmap(uncertainty_bins_clean, annot=False, cmap='Spectral', annot_kws={\"size\": fsize, \"weight\": \"bold\"}, ax=ax[0], cbar_ax=ax[1], norm=norm)\n",
    "                ax[0].invert_yaxis()\n",
    "                uacm_axsize = 30\n",
    "                if \"jetclass\" in key:\n",
    "                    uacm_axsize = 20\n",
    "                ax[0].set_xlabel(\"True Label\", fontsize=uacm_axsize)\n",
    "                ax[0].set_ylabel(\"Predicted Label + Uncertainty\".replace(\"Unc\", mult_label), fontsize=uacm_axsize)\n",
    "                for jj in np.arange(5,5*l_max, 5):\n",
    "                    ax[0].axhline(jj, linewidth=3, color='white', zorder=1)\n",
    "                    ax[0].axvline(jj//5, linewidth=3, color='white', zorder=1)\n",
    "                for i in range(l_max):\n",
    "                    ax[0].add_patch(\n",
    "                        plt.Rectangle((i, 5*i), 1, 5,\n",
    "                                      fill=False, edgecolor='black', linewidth=3, zorder=3, clip_on=False)\n",
    "                    )\n",
    "    \n",
    "                ax[0].set_yticks(np.arange(0, 5*l_max, 5))\n",
    "                ax[0].set_yticklabels(np.arange(0, l_max), rotation=0)\n",
    "                uacm_fsize = 24\n",
    "                if \"jetclass\" in key:\n",
    "                    uacm_fsize = 16\n",
    "                ax[0].tick_params(axis='both', which='major', labelsize=uacm_fsize)\n",
    "                cbar = heatmap.collections[0].colorbar\n",
    "                cbar.ax.tick_params(labelsize=uacm_fsize)\n",
    "                cbar.ax.yaxis.get_offset_text().set_fontsize(uacm_fsize)\n",
    "                if ('topdata' in key or 'jetnet' in key) and 'normal' in types:\n",
    "                    tick_values = cbar.get_ticks()\n",
    "                    cbar.set_ticklabels([f'{int(tick / 1000)}k' for tick in tick_values])\n",
    "                plt.tight_layout()\n",
    "                # plt.savefig(\"{}/labels_preds_unc_{}.pdf\".format(savefolder, filetype), dpi = 150, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "884ca032",
   "metadata": {},
   "source": [
    "visualize in altent space and compare and then do same with jetnet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba9f5b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "a4cfffe2",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b47b6671",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7912714b",
   "metadata": {},
   "source": [
    "**References**\n",
    "\n",
    "https://introtodeeplearning.com/2021/slides/6S191_MIT_DeepLearning_L7.pdf\n",
    "\n",
    "https://iopscience.iop.org/article/10.1088/2632-2153/ade51b\n",
    "\n",
    "https://arxiv.org/abs/1905.09638\n",
    "\n",
    "https://proceedings.mlr.press/v48/gal16.html\n",
    "\n",
    "https://arxiv.org/abs/1612.01474\n",
    "\n",
    "https://papers.nips.cc/paper_files/paper/2018/hash/a981f2b708044d6fb4a71a1463242520-Abstract.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ece5b4e",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
