{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/butler-julie/GDSVirtualTutorials/blob/main/072525_DecisionMaking/GDS_2_Gaussian_Proceses_GPyTorch_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1vmOUi5MTiav"
      },
      "source": [
        "# Gaussian processes\n",
        "\n",
        "The notebook for the GDS Tutorial, July 24, 2025\n",
        "- Instructor Sergei V. Kalinin\n",
        "- Examples adapted from: https://livebook.manning.com/book/bayesian-optimization-in-action/chapter-1/  \n",
        "- Several examples are based on ChatGPT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GnbTGh7J2Sah"
      },
      "outputs": [],
      "source": [
        "!pip install -qq gpytorch\n",
        "!pip install -qq botorch"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_MDCcnrLdUH7"
      },
      "source": [
        "# Conditioning distributions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6X8zjvYJqv0v"
      },
      "source": [
        "Visualizing a Multivariate Gaussian Distribution: Means, Variances, and Conditioning\n",
        "\n",
        "This code illustrates the structure of a **3-dimensional multivariate Gaussian distribution**, defined by a mean vector and a full covariance matrix. Specifically, we consider three random variables \\( A \\), \\( B \\), and \\( C \\), each following a joint Gaussian distribution:\n",
        "\n",
        "$$\n",
        "\\begin{bmatrix}\n",
        "A \\\\\n",
        "B \\\\\n",
        "C\n",
        "\\end{bmatrix}\n",
        "\\sim \\mathcal{N}(\\boldsymbol{\\mu}, \\Sigma)\n",
        "$$\n",
        "\n",
        "where:\n",
        "\n",
        "- \\( \\boldsymbol{\\mu} = [1, 4, 3] \\) is the mean vector for \\( A, B, C \\),\n",
        "- \\( \\Sigma \\) is the \\( 3 \\times 3 \\) covariance matrix containing the variances and covariances:\n",
        "  $$\n",
        "  \\Sigma =\n",
        "  \\begin{bmatrix}\n",
        "  1.5^2 & 0.99 & 0.1 \\\\\n",
        "  0.99 & 0.2^2 & 0.1 \\\\\n",
        "  0.1 & 0.1 & 1^2\n",
        "  \\end{bmatrix}\n",
        "  $$\n",
        "\n",
        "We then sample 1000 points from this distribution using `np.random.multivariate_normal`.\n",
        "\n",
        "The **visualization** consists of:\n",
        "- The **means** of the three variables, shown as purple dots.\n",
        "- The **standard deviations** (square roots of the diagonal of the covariance matrix), represented as vertical error bars around the means.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PgHgnYGHdWYU"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the parameters for the Gaussian distributions\n",
        "means = np.array([1, 4, 3])  # Means of A, B, C\n",
        "cov_matrix = np.array([\n",
        "    [1.5**2, 0.99, 0.1],  # Variance and covariance for A, B, C\n",
        "    [0.99, 0.2**2, 0.1],\n",
        "    [0.1, 0.1, 1**2]\n",
        "])  # Covariance matrix\n",
        "\n",
        "# Extract standard deviations (sqrt of diagonal elements of covariance matrix)\n",
        "std_devs = np.sqrt(np.diag(cov_matrix))\n",
        "\n",
        "# Generate samples from the multivariate Gaussian distribution\n",
        "num_samples = 1000\n",
        "data = np.random.multivariate_normal(means, cov_matrix, num_samples)\n",
        "A, B, C = data[:, 0], data[:, 1], data[:, 2]\n",
        "\n",
        "# Create the figure and axis\n",
        "fig, ax = plt.subplots(figsize=(8, 5))\n",
        "\n",
        "# Plot A, B, C as points on X-axis\n",
        "x_positions = np.arange(1, 4)  # Positions for A, B, C\n",
        "ax.scatter(x_positions, means, color=\"purple\", label=\"Means\")\n",
        "ax.errorbar(x_positions, means, yerr=std_devs, fmt=\"o\", color=\"black\", capsize=5, label=\"Standard Deviations\")\n",
        "\n",
        "# Set x-ticks and labels\n",
        "ax.set_xticks(x_positions)\n",
        "ax.set_xticklabels([\"A\", \"B\", \"C\"])\n",
        "ax.set_title(\"Means and Dispersions of A, B, and C\")\n",
        "ax.set_ylabel(\"Values\")\n",
        "ax.grid(True)\n",
        "ax.legend()\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jl7jFFAcmMH-"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the parameters for the Gaussian distributions\n",
        "means = np.array([1, 4, 3])  # Means of A, B, C\n",
        "cov_matrix = np.array([\n",
        "    [1.5**2, 0, 0.1],  # Variance and covariance for A, B, C\n",
        "    [0, 0.2**2, 0.1],\n",
        "    [0.1, 0.1, 1**2]\n",
        "])  # Covariance matrix\n",
        "\n",
        "# Extract standard deviations (sqrt of diagonal elements of covariance matrix)\n",
        "std_devs = np.sqrt(np.diag(cov_matrix))\n",
        "\n",
        "# Generate samples from the multivariate Gaussian distribution\n",
        "num_samples = 1000\n",
        "data = np.random.multivariate_normal(means, cov_matrix, num_samples)\n",
        "A, B, C = data[:, 0], data[:, 1], data[:, 2]\n",
        "\n",
        "# Generate a few joint samples from the distribution for visualization\n",
        "num_samples_to_plot = 15  # Number of joint samples to illustrate\n",
        "joint_samples = data[:num_samples_to_plot, :]  # Select the first few samples\n",
        "\n",
        "# Plot the joint examples of values drawn from the distribution\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot each joint sample as a connected line\n",
        "for i, sample in enumerate(joint_samples):\n",
        "    plt.plot([\"A\", \"B\", \"C\"], sample, marker=\"o\", label=f\"Sample {i+1}\")\n",
        "\n",
        "# Plot means with error bars derived from the covariance matrix\n",
        "plt.errorbar([\"A\", \"B\", \"C\"], means, yerr=std_devs, fmt=\"o\", color=\"black\", capsize=5, label=\"Means with Dispersion\")\n",
        "\n",
        "# Add labels, legend, and grid\n",
        "plt.title(\"Joint Examples and Mean Dispersions of A, B, and C\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.grid(True)\n",
        "# Uncomment to show the legend if desired\n",
        "# plt.legend()\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zFhw61BarUSM"
      },
      "source": [
        "📘 What about conditioning?\n",
        "\n",
        "In multivariate Gaussian distributions, we can **condition** on one variable to get the distribution of the others. For example, the **conditional distribution** of \\( A \\) given \\( B = b \\) (where \\( A \\) and \\( B \\) are jointly Gaussian) is also Gaussian, and its parameters are:\n",
        "\n",
        "**Conditional mean of \\( A \\) given \\( B = b \\):**\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[A \\mid B = b] = \\mu_A + \\Sigma_{AB} \\Sigma_{BB}^{-1} (b - \\mu_B)\n",
        "$$\n",
        "\n",
        "**Conditional variance of \\( A \\) given \\( B \\):**\n",
        "\n",
        "$$\n",
        "\\text{Var}[A \\mid B] = \\Sigma_{AA} - \\Sigma_{AB} \\Sigma_{BB}^{-1} \\Sigma_{BA}\n",
        "$$\n",
        "\n",
        "Where:\n",
        "\n",
        "- \\( \\mu_A, \\mu_B \\) are the means of \\( A \\) and \\( B \\),\n",
        "- \\( \\Sigma_{AA}, \\Sigma_{BB} \\) are their variances,\n",
        "- \\( \\Sigma_{AB} \\) is the covariance between \\( A \\) and \\( B \\).\n",
        "\n",
        "The same rules apply for conditioning on any subset of variables — this is very useful in **Bayesian inference**, **Kalman filters**, and **Gaussian processes**.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2egtlTYumsxq"
      },
      "outputs": [],
      "source": [
        "from scipy.stats import multivariate_normal\n",
        "\n",
        "# Conditioning the distribution on A = 1.2\n",
        "A_conditioned = 3\n",
        "\n",
        "# Extract the covariance matrix components\n",
        "Sigma = cov_matrix\n",
        "Sigma_aa = Sigma[0, 0]\n",
        "Sigma_ab = Sigma[0, 1:]\n",
        "Sigma_bb = Sigma[1:, 1:]\n",
        "\n",
        "# Compute the mean and covariance of the conditional distribution\n",
        "mean_b_given_a = means[1:] + Sigma_ab / Sigma_aa * (A_conditioned - means[0])\n",
        "cov_b_given_a = Sigma_bb - np.outer(Sigma_ab, Sigma_ab) / Sigma_aa\n",
        "\n",
        "# Generate samples from the posterior distribution\n",
        "posterior_samples = np.random.multivariate_normal(mean_b_given_a, cov_b_given_a, num_samples_to_plot)\n",
        "\n",
        "# Plot the posterior samples\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot each posterior sample as a connected line (with A fixed at 1.2)\n",
        "for i, sample in enumerate(posterior_samples):\n",
        "    plt.plot([\"A\", \"B\", \"C\"], [A_conditioned] + sample.tolist(), marker=\"o\", label=f\"Posterior Sample {i+1}\")\n",
        "\n",
        "# Plot the posterior mean with error bars\n",
        "plt.errorbar(\n",
        "    [\"A\", \"B\", \"C\"],\n",
        "    [A_conditioned] + mean_b_given_a.tolist(),\n",
        "    yerr=[0] + [np.sqrt(cov_b_given_a[i, i]) for i in range(2)],\n",
        "    fmt=\"o\",\n",
        "    color=\"black\",\n",
        "    capsize=5,\n",
        "    label=\"Posterior Mean with Dispersion\"\n",
        ")\n",
        "\n",
        "# Add labels, legend, and grid\n",
        "plt.title(\"Posterior Samples and Mean Dispersions (Conditioned on A = 1.2)\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.grid(True)\n",
        "#plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-1kIrfTdnihn"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the parameters for the Gaussian distributions\n",
        "means = np.array([1, 4, 3])  # Means of A, B, C\n",
        "cov_matrix = np.array([\n",
        "    [1.5**2, 0.9, 0.1],  # Variance and covariance for A, B, C\n",
        "    [0.9, 0.2**2, 0.1],\n",
        "    [0.1, 0.1, 1**2]\n",
        "])  # Covariance matrix\n",
        "\n",
        "# Extract standard deviations (sqrt of diagonal elements of covariance matrix)\n",
        "std_devs = np.sqrt(np.diag(cov_matrix))\n",
        "\n",
        "# Generate samples from the multivariate Gaussian distribution\n",
        "num_samples = 1000\n",
        "data = np.random.multivariate_normal(means, cov_matrix, num_samples)\n",
        "A, B, C = data[:, 0], data[:, 1], data[:, 2]\n",
        "\n",
        "# Generate a few joint samples from the distribution for visualization\n",
        "num_samples_to_plot = 15  # Number of joint samples to illustrate\n",
        "joint_samples = data[:num_samples_to_plot, :]  # Select the first few samples\n",
        "\n",
        "# Conditioning the distribution on C = 6\n",
        "C_conditioned = 6\n",
        "\n",
        "# Extract the covariance matrix components for conditioning on C\n",
        "Sigma_cc = Sigma[2, 2]\n",
        "Sigma_cb = Sigma[2, :2]\n",
        "Sigma_bb = Sigma[:2, :2]\n",
        "\n",
        "# Compute the mean and covariance of the conditional distribution\n",
        "mean_b_given_c = means[:2] + Sigma_cb / Sigma_cc * (C_conditioned - means[2])\n",
        "cov_b_given_c = Sigma_bb - np.outer(Sigma_cb, Sigma_cb) / Sigma_cc\n",
        "\n",
        "# Generate samples from the posterior distribution\n",
        "posterior_samples_c = np.random.multivariate_normal(mean_b_given_c, cov_b_given_c, num_samples_to_plot)\n",
        "\n",
        "# Plot the posterior samples\n",
        "plt.figure(figsize=(8, 5))\n",
        "\n",
        "# Plot each posterior sample as a connected line (with C fixed at 6)\n",
        "for i, sample in enumerate(posterior_samples_c):\n",
        "    plt.plot([\"A\", \"B\", \"C\"], sample.tolist() + [C_conditioned], marker=\"o\", label=f\"Posterior Sample {i+1}\")\n",
        "\n",
        "# Plot the posterior mean with error bars\n",
        "plt.errorbar(\n",
        "    [\"A\", \"B\", \"C\"],\n",
        "    mean_b_given_c.tolist() + [C_conditioned],\n",
        "    yerr=[np.sqrt(cov_b_given_c[i, i]) for i in range(2)] + [0],\n",
        "    fmt=\"o\",\n",
        "    color=\"black\",\n",
        "    capsize=5,\n",
        "    label=\"Posterior Mean with Dispersion\"\n",
        ")\n",
        "\n",
        "# Add labels, legend, and grid\n",
        "plt.title(\"Posterior Samples and Mean Dispersions (Conditioned on C = 6)\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "me3mygQttMVH"
      },
      "source": [
        "🔍 Gaussian Conditioning: How Posterior of B and C Changes with the Value of A\n",
        "\n",
        "This code explores how the **posterior (conditional) distribution of variables B and C** evolves as we **condition on different values of variable A**, in the context of a **multivariate Gaussian distribution**.\n",
        "We begin with a **correlation matrix** describing the relationships between three Gaussian variables \\( A \\), \\( B \\), and \\( C \\), and then convert this correlation matrix into a **covariance matrix** using the provided standard deviations:\n",
        "\n",
        "$$\n",
        "\\Sigma = \\text{diag}(\\sigma) \\cdot \\text{Corr} \\cdot \\text{diag}(\\sigma)\n",
        "$$\n",
        "\n",
        "where \\( \\sigma = [1.5, 0.2, 1.0] \\) are the standard deviations of \\( A, B, C \\). The mean vector is given as:\n",
        "\n",
        "$$\n",
        "\\mu = [1, 4, 3]\n",
        "$$\n",
        "\n",
        "We then compute the **conditional distribution** of \\( [B, C] \\) given that \\( A = a \\), for many values of \\( a \\) ranging from 0 to 5.\n",
        "\n",
        "---\n",
        "\n",
        "🧮 Conditional Gaussian Formulas Used\n",
        "\n",
        "Given a joint distribution over \\( A \\) and \\( [B, C] \\), the conditional distribution of \\( [B, C] \\mid A = a \\) is again Gaussian:\n",
        "\n",
        "**Conditional mean:**\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[B, C \\mid A = a] = \\mu_{BC} + \\Sigma_{BC,A} \\Sigma_{AA}^{-1} (a - \\mu_A)\n",
        "$$\n",
        "\n",
        "**Conditional covariance:**\n",
        "\n",
        "$$\n",
        "\\text{Cov}[B, C \\mid A] = \\Sigma_{BC,BC} - \\Sigma_{BC,A} \\Sigma_{AA}^{-1} \\Sigma_{A,BC}\n",
        "$$\n",
        "\n",
        "These formulas are used in each iteration of the loop, plugging in the current value of \\( A \\) being conditioned on.\n",
        "\n",
        "---\n",
        "\n",
        "📈 What the plot shows:\n",
        "\n",
        "- The **solid blue and green lines** represent the **posterior means** of \\( B \\) and \\( C \\), respectively, as a function of the conditioned value \\( A \\).\n",
        "- The **shaded regions** represent **±1 standard deviation** of the conditional (posterior) distributions — that is, the spread or uncertainty in \\( B \\) and \\( C \\) after observing \\( A = a \\).\n",
        "\n",
        "As we change the value of \\( A \\), the means of \\( B \\) and \\( C \\) shift according to the correlation structure between the variables. The posterior variances remain constant in this case because we are only conditioning on one variable and keeping its variance fixed.\n",
        "\n",
        "---\n",
        "\n",
        "This code provides an intuition for how **observing one variable (A)** in a multivariate Gaussian impacts our **belief (mean and uncertainty)** about the others (B and C)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bPLjd1q4oHoE"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#try 0.9 vs. 0.09\n",
        "\n",
        "# Define the initial correlation matrix and convert it to a covariance matrix\n",
        "correlation_matrix = np.array([\n",
        "    [1.0, 0.09, 0],  # Correlation coefficients for A, B, C\n",
        "    [0.09, 0.4, 0.5],\n",
        "    [0, 0.5, 0.4]\n",
        "])\n",
        "\n",
        "# Define standard deviations for A, B, C\n",
        "std_devs = np.array([1.5, 1.2, 1.0])\n",
        "\n",
        "# Convert correlation matrix to covariance matrix\n",
        "cov_matrix = np.diag(std_devs) @ correlation_matrix @ np.diag(std_devs)\n",
        "\n",
        "# Define the means for A, B, C\n",
        "means = np.array([1, 4, 3])\n",
        "\n",
        "# Define the range of A to condition on\n",
        "A_conditioned_values = np.linspace(0, 5, 100)\n",
        "\n",
        "# Initialize arrays to store posterior means and standard deviations for B and C\n",
        "mean_b_given_a_values = []\n",
        "mean_c_given_a_values = []\n",
        "std_b_given_a_values = []\n",
        "std_c_given_a_values = []\n",
        "\n",
        "# Loop over the values of A to compute conditional statistics for B and C\n",
        "for A_conditioned in A_conditioned_values:\n",
        "    # Mean of B and C given A\n",
        "    mean_given_a = means[1:] + (cov_matrix[1:, 0] / cov_matrix[0, 0]) * (A_conditioned - means[0])\n",
        "    # Covariance of B and C given A\n",
        "    cov_given_a = cov_matrix[1:, 1:] - np.outer(cov_matrix[1:, 0], cov_matrix[1:, 0]) / cov_matrix[0, 0]\n",
        "\n",
        "    # Append mean and standard deviations\n",
        "    mean_b_given_a_values.append(mean_given_a[0])\n",
        "    mean_c_given_a_values.append(mean_given_a[1])\n",
        "    std_b_given_a_values.append(np.sqrt(cov_given_a[0, 0]))\n",
        "    std_c_given_a_values.append(np.sqrt(cov_given_a[1, 1]))\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "mean_b_given_a_values = np.array(mean_b_given_a_values)\n",
        "mean_c_given_a_values = np.array(mean_c_given_a_values)\n",
        "std_b_given_a_values = np.array(std_b_given_a_values)\n",
        "std_c_given_a_values = np.array(std_c_given_a_values)\n",
        "\n",
        "# Plot the mean and posterior dispersion for B and C\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Mean and dispersion for B\n",
        "plt.plot(A_conditioned_values, mean_b_given_a_values, label=\"Mean of B\", color=\"blue\")\n",
        "plt.fill_between(A_conditioned_values,\n",
        "                 mean_b_given_a_values - std_b_given_a_values,\n",
        "                 mean_b_given_a_values + std_b_given_a_values,\n",
        "                 color=\"blue\", alpha=0.2, label=\"Posterior Dispersion of B\")\n",
        "\n",
        "# Mean and dispersion for C\n",
        "plt.plot(A_conditioned_values, mean_c_given_a_values, label=\"Mean of C\", color=\"green\")\n",
        "plt.fill_between(A_conditioned_values,\n",
        "                 mean_c_given_a_values - std_c_given_a_values,\n",
        "                 mean_c_given_a_values + std_c_given_a_values,\n",
        "                 color=\"green\", alpha=0.2, label=\"Posterior Dispersion of C\")\n",
        "\n",
        "# Labels, legend, and grid\n",
        "plt.title(\"Posterior Mean and Dispersion of B and C as a Function of A\")\n",
        "plt.xlabel(\"Conditioned A\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg8SJ86MsBPI"
      },
      "source": [
        "🔄 Conditioning in Multivariate Gaussian: Effect of Variance in the Conditioned Variable\n",
        "\n",
        "This code illustrates a key concept in multivariate Gaussian distributions: **how the conditional distribution of some variables (B and C) changes depending on the variance of another variable (A)** we are conditioning on.\n",
        "\n",
        "We start with a 3-dimensional Gaussian distribution over variables \\( A, B, C \\), specified by:\n",
        "\n",
        "- A **mean vector** \\( \\boldsymbol{\\mu} = [1, 4, 3] \\)\n",
        "- A **covariance matrix**:\n",
        "\n",
        "$$\n",
        "\\Sigma =\n",
        "\\begin{bmatrix}\n",
        "\\text{Var}(A) & \\text{Cov}(A, B) & \\text{Cov}(A, C) \\\\\n",
        "\\text{Cov}(B, A) & \\text{Var}(B) & \\text{Cov}(B, C) \\\\\n",
        "\\text{Cov}(C, A) & \\text{Cov}(C, B) & \\text{Var}(C)\n",
        "\\end{bmatrix}\n",
        "$$\n",
        "\n",
        "In this code, we explore what happens to the conditional mean and standard deviation (dispersion) of \\( B \\) and \\( C \\) **as the variance of \\( A \\) decreases**, ranging from 4 down to 0.1.\n",
        "\n",
        "---\n",
        "\n",
        "🧮 What is being computed?\n",
        "\n",
        "For each value of \\( \\text{Var}(A) \\), we compute the **conditional distribution of \\( B, C \\mid A = 2 \\)** using the Gaussian conditioning formulas:\n",
        "\n",
        "**Conditional mean:**\n",
        "\n",
        "Let \\( \\mu = \\begin{bmatrix} \\mu_A \\\\ \\mu_{BC} \\end{bmatrix} \\), and \\( \\Sigma = \\begin{bmatrix} \\Sigma_{AA} & \\Sigma_{A,BC} \\\\ \\Sigma_{BC,A} & \\Sigma_{BC,BC} \\end{bmatrix} \\)\n",
        "\n",
        "Then the conditional mean of \\( B \\) and \\( C \\) given \\( A = a \\) is:\n",
        "\n",
        "$$\n",
        "\\mathbb{E}[B, C \\mid A = a] = \\mu_{BC} + \\Sigma_{BC,A} \\Sigma_{AA}^{-1} (a - \\mu_A)\n",
        "$$\n",
        "\n",
        "**Conditional covariance:**\n",
        "\n",
        "$$\n",
        "\\text{Cov}[B, C \\mid A] = \\Sigma_{BC,BC} - \\Sigma_{BC,A} \\Sigma_{AA}^{-1} \\Sigma_{A,BC}\n",
        "$$\n",
        "\n",
        "These are computed for each value of \\( \\text{Var}(A) \\), while keeping the covariances between variables fixed.\n",
        "\n",
        "---\n",
        "\n",
        "📈 What the plot shows:\n",
        "\n",
        "- The **solid lines** show the **conditional means** of \\( B \\) (blue) and \\( C \\) (green) as the variance of \\( A \\) changes.\n",
        "- The **shaded regions** around each mean represent **±1 standard deviation** of the conditional distribution, derived from the conditional covariance.\n",
        "- As \\( \\text{Var}(A) \\to 0 \\), the influence of \\( A = 2 \\) becomes dominant, and the conditional variance of \\( B \\) and \\( C \\) shrinks (i.e., the distribution collapses to a line).\n",
        "- As \\( \\text{Var}(A) \\) increases, the conditional influence of \\( A \\) weakens, and \\( B \\), \\( C \\) revert toward their marginal distributions.\n",
        "\n",
        "---\n",
        "\n",
        "This is an illustration of how **information content in conditioning variables** (captured by their variance) impacts uncertainty in the conditional distribution. It mirrors what happens in Bayesian inference, Kalman filtering, and Gaussian process regression.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "quwJwTl5vItx"
      },
      "outputs": [],
      "source": [
        "# Initialize all parameters at the beginning\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Means of A, B, C\n",
        "means = np.array([1, 4, 3])\n",
        "\n",
        "# Covariance matrix of A, B, C\n",
        "cov_matrix = np.array([\n",
        "    [1.5**2, 0.9, 0.1],\n",
        "    [0.9, 2**2, 0.1],\n",
        "    [0.1, 0.1, 1**2]\n",
        "])\n",
        "\n",
        "# Range of variances for A (from 4 to a small positive value)\n",
        "variance_a_values = np.linspace(4, 0.1, 100)\n",
        "\n",
        "# Fixed value of A\n",
        "A_conditioned = 2\n",
        "\n",
        "# Initialize arrays to store posterior means and standard deviations for B and C\n",
        "mean_b_given_a_values = []\n",
        "mean_c_given_a_values = []\n",
        "std_b_given_a_values = []\n",
        "std_c_given_a_values = []\n",
        "\n",
        "# Main calculation loop for conditional statistics\n",
        "for variance_a in variance_a_values:\n",
        "    # Update the variance of A in the covariance matrix\n",
        "    cov_matrix_updated = cov_matrix.copy()\n",
        "    cov_matrix_updated[0, 0] = variance_a  # Update variance of A\n",
        "\n",
        "    # Compute the mean of B and C given A\n",
        "    mean_given_a = means[1:] + (cov_matrix_updated[1:, 0] / variance_a) * (A_conditioned - means[0])\n",
        "\n",
        "    # Compute the covariance of B and C given A\n",
        "    cov_given_a = cov_matrix_updated[1:, 1:] - np.outer(cov_matrix_updated[1:, 0], cov_matrix_updated[1:, 0]) / variance_a\n",
        "\n",
        "    # Append mean and standard deviations\n",
        "    mean_b_given_a_values.append(mean_given_a[0])\n",
        "    mean_c_given_a_values.append(mean_given_a[1])\n",
        "    std_b_given_a_values.append(np.sqrt(cov_given_a[0, 0]))\n",
        "    std_c_given_a_values.append(np.sqrt(cov_given_a[1, 1]))\n",
        "\n",
        "# Convert lists to numpy arrays\n",
        "mean_b_given_a_values = np.array(mean_b_given_a_values)\n",
        "mean_c_given_a_values = np.array(mean_c_given_a_values)\n",
        "std_b_given_a_values = np.array(std_b_given_a_values)\n",
        "std_c_given_a_values = np.array(std_c_given_a_values)\n",
        "\n",
        "# Plot the conditional means and dispersions for B and C\n",
        "plt.figure(figsize=(10, 6))\n",
        "\n",
        "# Plot mean and dispersion for B\n",
        "plt.plot(variance_a_values, mean_b_given_a_values, label=\"Mean of B\", color=\"blue\")\n",
        "plt.fill_between(variance_a_values,\n",
        "                 mean_b_given_a_values - std_b_given_a_values,\n",
        "                 mean_b_given_a_values + std_b_given_a_values,\n",
        "                 color=\"blue\", alpha=0.2, label=\"Dispersion of B\")\n",
        "\n",
        "# Plot mean and dispersion for C\n",
        "plt.plot(variance_a_values, mean_c_given_a_values, label=\"Mean of C\", color=\"green\")\n",
        "plt.fill_between(variance_a_values,\n",
        "                 mean_c_given_a_values - std_c_given_a_values,\n",
        "                 mean_c_given_a_values + std_c_given_a_values,\n",
        "                 color=\"green\", alpha=0.2, label=\"Dispersion of C\")\n",
        "\n",
        "# Add labels, legend, and grid\n",
        "plt.title(\"Conditional Mean and Dispersion of B and C as Variance of A Changes\")\n",
        "plt.xlabel(\"Variance of A\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oYZLadqAuuOk"
      },
      "source": [
        "# From 3 to Many: Gaussian Process"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3gI5yXQpyIGo"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Initialize parameters\n",
        "num_vars = 20  # Number of variables\n",
        "means = np.linspace(10, 20, num_vars)  # Means are linearly spaced from 10 to 20\n",
        "\n",
        "# Initialize correlation matrix\n",
        "correlation_matrix = np.zeros((num_vars, num_vars))\n",
        "for i in range(num_vars):\n",
        "    for j in range(num_vars):\n",
        "        if i == j:\n",
        "            correlation_matrix[i, j] = 1.0  # Main diagonal\n",
        "        elif abs(i - j) == 1:\n",
        "            correlation_matrix[i, j] = 0.9  # First off-diagonal\n",
        "        elif abs(i - j) == 2:\n",
        "            correlation_matrix[i, j] = 0.5  # Second off-diagonal\n",
        "        elif abs(i - j) == 3:\n",
        "            correlation_matrix[i, j] = 0.2  # Second off-diagonal\n",
        "\n",
        "# Ensure the matrix is symmetric\n",
        "correlation_matrix = (correlation_matrix + correlation_matrix.T) / 2\n",
        "\n",
        "# Generate covariance matrix by scaling with arbitrary variances\n",
        "variances = np.linspace(1, 1, num_vars)  # Variances for each variable\n",
        "covariance_matrix = correlation_matrix * np.outer(variances, variances)\n",
        "\n",
        "# Plot the correlation matrix as a heatmap\n",
        "plt.figure(figsize=(8, 6))\n",
        "plt.imshow(correlation_matrix, cmap=\"coolwarm\", interpolation=\"nearest\")\n",
        "plt.colorbar(label=\"Correlation Coefficient\")\n",
        "plt.title(\"Correlation Matrix Heatmap\")\n",
        "plt.xlabel(\"Variable Index\")\n",
        "plt.ylabel(\"Variable Index\")\n",
        "plt.grid(False)\n",
        "plt.show()\n",
        "\n",
        "# Plot means and dispersions\n",
        "plt.figure(figsize=(10, 6))\n",
        "x_indices = np.arange(1, num_vars + 1)\n",
        "plt.errorbar(x_indices, means, yerr=np.sqrt(variances), fmt=\"o\", color=\"black\", capsize=5, label=\"Means with Dispersions\")\n",
        "plt.title(\"Means and Dispersions for Variables\")\n",
        "plt.xlabel(\"Variable Index\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b6g6ClHCytkC"
      },
      "outputs": [],
      "source": [
        "# Conditioning on variable 10 (index 9 in zero-based indexing) to have a value of 18\n",
        "conditioned_index = 9\n",
        "conditioned_value = 18\n",
        "\n",
        "# Compute the conditional mean and covariance\n",
        "# Partition the covariance matrix and mean vector\n",
        "indices = np.arange(num_vars)\n",
        "other_indices = indices[indices != conditioned_index]\n",
        "\n",
        "mean_cond = means[other_indices] + (\n",
        "    covariance_matrix[other_indices, conditioned_index]\n",
        "    / covariance_matrix[conditioned_index, conditioned_index]\n",
        ") * (conditioned_value - means[conditioned_index])\n",
        "\n",
        "cov_cond = (\n",
        "    covariance_matrix[np.ix_(other_indices, other_indices)]\n",
        "    - np.outer(\n",
        "        covariance_matrix[other_indices, conditioned_index],\n",
        "        covariance_matrix[other_indices, conditioned_index],\n",
        "    )\n",
        "    / covariance_matrix[conditioned_index, conditioned_index]\n",
        ")\n",
        "\n",
        "# Create a full conditioned mean and variances array for all variables\n",
        "conditioned_means = means.copy()\n",
        "conditioned_means[other_indices] = mean_cond\n",
        "conditioned_means[conditioned_index] = conditioned_value\n",
        "\n",
        "conditioned_variances = np.zeros(num_vars)\n",
        "conditioned_variances[conditioned_index] = 0  # Variance for conditioned variable is 0\n",
        "conditioned_variances[other_indices] = np.diag(cov_cond)\n",
        "\n",
        "# Plot the conditioned means and dispersions\n",
        "plt.figure(figsize=(10, 6))\n",
        "x_indices = np.arange(1, num_vars + 1)\n",
        "plt.errorbar(\n",
        "    x_indices,\n",
        "    conditioned_means,\n",
        "    yerr=np.sqrt(conditioned_variances),\n",
        "    fmt=\"o\",\n",
        "    color=\"black\",\n",
        "    capsize=5,\n",
        "    label=\"Conditioned Means with Dispersions\",\n",
        ")\n",
        "plt.title(\"Means and Dispersions After Conditioning on Variable 10\")\n",
        "plt.xlabel(\"Variable Index\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZcHfGN3V0nbq"
      },
      "outputs": [],
      "source": [
        "# Correcting the sampling to ensure variable 10 is fixed at 18 in the conditioned samples\n",
        "# Original samples\n",
        "original_samples = np.random.multivariate_normal(means, covariance_matrix, num_samples)\n",
        "\n",
        "# Conditioned samples\n",
        "conditioned_samples = np.zeros((num_samples, num_vars))\n",
        "\n",
        "# Set variable 10 to 18 for all conditioned samples\n",
        "conditioned_samples[:, conditioned_index] = conditioned_value\n",
        "\n",
        "# Sample the remaining variables from the conditioned distribution\n",
        "for i in range(num_samples):\n",
        "    conditioned_samples[i, other_indices] = np.random.multivariate_normal(mean_cond, cov_cond)\n",
        "\n",
        "# Plot the original and conditioned samples\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot original samples\n",
        "for i, sample in enumerate(original_samples):\n",
        "    plt.plot(\n",
        "        np.arange(1, num_vars + 1),\n",
        "        sample,\n",
        "        marker=\"o\",\n",
        "        linestyle=\"--\",\n",
        "        label=\"Original Samples\" if i == 0 else \"\",\n",
        "        color=\"blue\",\n",
        "        alpha=0.6,\n",
        "    )\n",
        "\n",
        "# Plot conditioned samples\n",
        "for i, sample in enumerate(conditioned_samples):\n",
        "    plt.plot(\n",
        "        np.arange(1, num_vars + 1),\n",
        "        sample,\n",
        "        marker=\"o\",\n",
        "        linestyle=\"-\",\n",
        "        label=\"Conditioned Samples\" if i == 0 else \"\",\n",
        "        color=\"orange\",\n",
        "        alpha=0.6,\n",
        "    )\n",
        "\n",
        "# Add plot details\n",
        "plt.title(\"Functions Drawn from Original and Conditioned Distributions\")\n",
        "plt.xlabel(\"Variable Index\")\n",
        "plt.ylabel(\"Values\")\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_FmwjBfwIHD"
      },
      "source": [
        "# GPyTorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8WIuJcaLw39z"
      },
      "outputs": [],
      "source": [
        "!pip install -qq gpytorch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AVXd1uyW0twe"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def forrester_1d(x):\n",
        "    y = -((x + 1) ** 2) * torch.sin(2 * x + 2) / 5 + 1\n",
        "    return y.squeeze(-1)\n",
        "\n",
        "xs = torch.linspace(-3, 3, 101).unsqueeze(1)\n",
        "ys = forrester_1d(xs)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "train_x = torch.rand(size=(3, 1)) * 6 - 3\n",
        "train_y = forrester_1d(train_x)\n",
        "\n",
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(xs, ys, label=\"objective\", c=\"r\")\n",
        "plt.scatter(train_x, train_y, marker=\"x\", c=\"k\", label=\"observations\")\n",
        "\n",
        "plt.legend(fontsize=15);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cXd9IgXt1D16"
      },
      "outputs": [],
      "source": [
        "import gpytorch\n",
        "\n",
        "class BaseGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super().__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ZeroMean()\n",
        "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1mi38im2WoX"
      },
      "outputs": [],
      "source": [
        "# declare the GP\n",
        "lengthscale = 1\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = BaseGPModel(None, None, likelihood)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.covar_module.lengthscale = lengthscale\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j9zm93t72qbU"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    predictive_distribution = likelihood(model(xs))\n",
        "    predictive_mean = predictive_distribution.mean\n",
        "    predictive_lower, predictive_upper = predictive_distribution.confidence_region()\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    samples = predictive_distribution.sample(torch.Size([5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXqHsdKf2uIZ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(xs, predictive_mean.detach(), label=\"mean\")\n",
        "plt.fill_between(xs.flatten(), predictive_upper, predictive_lower, alpha=0.3, label=\"95% CI\")\n",
        "plt.plot(xs, samples[0, :], alpha=0.5, label=\"samples\")\n",
        "\n",
        "for i in range(1, samples.shape[0]):\n",
        "    plt.plot(xs, samples[i, :], alpha=0.5)\n",
        "\n",
        "plt.ylim(-3, 3)\n",
        "\n",
        "plt.legend(fontsize=15);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4looYm_E2yLa"
      },
      "outputs": [],
      "source": [
        "# declare the GP\n",
        "lengthscale = 1\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = BaseGPModel(train_x, train_y, likelihood)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.covar_module.lengthscale = lengthscale\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nLsK9n_K3BIM"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    predictive_distribution = likelihood(model(xs))\n",
        "    predictive_mean = predictive_distribution.mean\n",
        "    predictive_lower, predictive_upper = predictive_distribution.confidence_region()\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    samples = predictive_distribution.sample(torch.Size([5]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f6Ak7-KU3D7t"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(8, 6))\n",
        "\n",
        "plt.plot(xs, ys, label=\"objective\", c=\"r\")\n",
        "plt.scatter(train_x, train_y, marker=\"x\", c=\"k\", label=\"observations\")\n",
        "\n",
        "plt.plot(xs, predictive_mean, label=\"mean\")\n",
        "plt.fill_between(xs.flatten(), predictive_upper, predictive_lower, alpha=0.3, label=\"95\\% CI\")\n",
        "\n",
        "plt.plot(xs, samples[0, :], alpha=0.5, label=\"samples\")\n",
        "\n",
        "for i in range(1, samples.shape[0]):\n",
        "    plt.plot(xs, samples[i, :], alpha=0.5)\n",
        "\n",
        "plt.legend(fontsize=15);"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS9iwbxK3Ko1"
      },
      "outputs": [],
      "source": [
        "train_x = torch.tensor([[0.0, 0.0], [1.0, 2.0], [-1.0, 1.0]])\n",
        "\n",
        "train_y = torch.tensor([0.0, -1.0, 0.5])\n",
        "grid_x = torch.linspace(-3, 3, 101)\n",
        "\n",
        "grid_x1, grid_x2 = torch.meshgrid(grid_x, grid_x, indexing=\"ij\")\n",
        "xs = torch.vstack([grid_x1.flatten(), grid_x2.flatten()]).transpose(-1, -2)\n",
        "# declare the GP\n",
        "lengthscale = 1\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = BaseGPModel(train_x, train_y, likelihood)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.covar_module.lengthscale = lengthscale\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j5hLm9fA3QZL"
      },
      "outputs": [],
      "source": [
        "with torch.no_grad():\n",
        "    predictive_distribution = likelihood(model(xs))\n",
        "    predictive_mean = predictive_distribution.mean\n",
        "    predictive_stddev = predictive_distribution.stddev\n",
        "plt.rcParams[\"image.cmap\"] = \"magma\"\n",
        "fig, ax = plt.subplots(1, 2, figsize=(16, 6))\n",
        "\n",
        "c = ax[0].imshow(\n",
        "    predictive_mean.detach().reshape(101, 101).transpose(-1, -2),\n",
        "    origin=\"lower\",\n",
        "    extent=[-3, 3, -3, 3],\n",
        ")\n",
        "plt.colorbar(c, ax=ax[0])\n",
        "\n",
        "ax[0].scatter(train_x[:, 0], train_x[:, 1], c=\"blue\", marker=\"D\", s=100)\n",
        "\n",
        "ax[0].set_title(\"mean\")\n",
        "\n",
        "c = ax[1].imshow(\n",
        "    predictive_stddev.detach().reshape(101, 101).transpose(-1, -2),\n",
        "    origin=\"lower\",\n",
        "    extent=[-3, 3, -3, 3],\n",
        ")\n",
        "plt.colorbar(c, ax=ax[1])\n",
        "\n",
        "ax[1].scatter(\n",
        "    train_x[:, 0], train_x[:, 1], c=\"blue\", marker=\"D\", s=100, label=\"observations\"\n",
        ")\n",
        "\n",
        "ax[1].set_title(\"standard deviation\")\n",
        "\n",
        "ax[1].legend()\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dkBXAWcLPoj5"
      },
      "source": [
        "# Kernel engineering"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HdZ5xJSg0Iei"
      },
      "source": [
        "### Define kernel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWrRdQniPsF4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import gpytorch\n",
        "\n",
        "from math import pi\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "plt.style.use(\"bmh\")\n",
        "plt.rcParams[\"image.cmap\"] = \"Blues_r\"\n",
        "\n",
        "from tqdm.notebook import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3qhSvqeQI6i"
      },
      "outputs": [],
      "source": [
        "def forrester_1d(x):\n",
        "    # a modification of https://www.sfu.ca/~ssurjano/forretal08.html\n",
        "    y = -((x + 1) ** 2) * torch.sin(2 * x + 2) / 5 + 1\n",
        "    return y.squeeze(-1)\n",
        "\n",
        "\n",
        "def visualize_gp_belief(model, likelihood, num_samples=5):\n",
        "    with torch.no_grad():\n",
        "        predictive_distribution = likelihood(model(xs))\n",
        "        predictive_mean = predictive_distribution.mean\n",
        "        predictive_upper, predictive_lower = predictive_distribution.confidence_region()\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "\n",
        "    plt.plot(xs, ys, label=\"objective\", c=\"r\")\n",
        "    plt.scatter(train_x, train_y, marker=\"x\", c=\"k\", label=\"observations\")\n",
        "\n",
        "    plt.plot(xs, predictive_mean, label=\"mean\")\n",
        "    plt.fill_between(\n",
        "        xs.flatten(), predictive_upper, predictive_lower, alpha=0.3, label=\"95% CI\"\n",
        "    )\n",
        "\n",
        "    torch.manual_seed(0)\n",
        "    for i in range(num_samples):\n",
        "        plt.plot(xs, predictive_distribution.sample(), alpha=0.5, linewidth=2)\n",
        "\n",
        "    plt.legend(fontsize=15)\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXOi1AQJQMjE"
      },
      "outputs": [],
      "source": [
        "xs = torch.linspace(-3, 3, 101).unsqueeze(1)\n",
        "ys = forrester_1d(xs)\n",
        "\n",
        "torch.manual_seed(0)\n",
        "train_x = torch.rand(size=(3, 1)) * 6 - 3\n",
        "train_y = forrester_1d(train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k3ui-ZfwQQi_"
      },
      "outputs": [],
      "source": [
        "class ScaleGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super().__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ZeroMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(gpytorch.kernels.RBFKernel())\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fcEct6p4y2f7"
      },
      "source": [
        "This code sets up a Gaussian Process (GP) regression model using the `gpytorch` library, with fixed hyperparameters for educational or diagnostic purposes. The `lengthscale`, `outputscale`, and `noise` variables define the behavior of the GP kernel and likelihood. The lengthscale controls how quickly the function can vary with input — a smaller value leads to sharper, more rapid changes, while a larger value makes the GP smoother. The outputscale determines the vertical amplitude or variability of the GP's predictions. The noise term defines the assumed observational noise level in the data; setting it to a small value like `1e-4` approximates a nearly noise-free interpolation.\n",
        "\n",
        "The model is created using `ScaleGPModel`, a custom GP class based on `gpytorch.models.ExactGP` which wraps a scaled kernel such as RBF. The `GaussianLikelihood` assumes that observations are real-valued and corrupted by Gaussian noise. Instead of learning the hyperparameters through optimization, the code manually sets them by assigning values to the model's kernel and likelihood objects. This allows the user to explicitly control the GP's behavior and observe how the shape of the predictive distribution changes.\n",
        "\n",
        "After setting the parameters, the model is switched to evaluation mode using `.eval()`, which disables training-specific behavior like gradient tracking. Finally, the `visualize_gp_belief` function is called, which likely plots the GP’s posterior mean and uncertainty (typically shown as shaded regions), along with the training data points.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TkbIaWRvQZ1-"
      },
      "outputs": [],
      "source": [
        "# declare the GP\n",
        "lengthscale = 1  # 0.3, 1, 3\n",
        "outputscale = 0.03  # 0.3, 1, 3\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = ScaleGPModel(train_x, train_y, likelihood)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.covar_module.base_kernel.lengthscale = lengthscale\n",
        "model.covar_module.outputscale = outputscale\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "\n",
        "visualize_gp_belief(model, likelihood)\n",
        "#model.likelihood.noise"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YyERbmch0NHp"
      },
      "source": [
        "### Train kernel parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Kjd9dkQ4XFGO"
      },
      "outputs": [],
      "source": [
        "# train the hyperparameter (the constant)\n",
        "optimizer = torch.optim.Adam(model.covar_module.parameters(), lr=0.01)\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "model.train()\n",
        "likelihood.train()\n",
        "\n",
        "losses = []\n",
        "lengthscales = []\n",
        "outputscales = []\n",
        "\n",
        "for i in tqdm(range(500)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    lengthscales.append(model.covar_module.base_kernel.lengthscale.item())\n",
        "    outputscales.append(model.covar_module.outputscale.item())\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jZMTSYgwXGhA"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_ylabel(\"negative marginal log likelihood\")\n",
        "\n",
        "ax[1].plot(lengthscales)\n",
        "ax[1].set_ylabel(\"lengthscale\")\n",
        "\n",
        "ax[2].plot(outputscales)\n",
        "ax[2].set_ylabel(\"outputscale\")\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fWhu7_QiY3qH"
      },
      "outputs": [],
      "source": [
        "visualize_gp_belief(model, likelihood)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZyY19iOI0YRg"
      },
      "source": [
        "### Experiment with kernel types"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wdoeQFNaZAWm"
      },
      "outputs": [],
      "source": [
        "class RBFGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super().__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ZeroMean()\n",
        "        self.covar_module = gpytorch.kernels.RBFKernel()\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "toN-9o9mZDTZ"
      },
      "outputs": [],
      "source": [
        "# declare the GP\n",
        "lengthscale = 1\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = RBFGPModel(train_x, train_y, likelihood)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.covar_module.lengthscale = lengthscale\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "\n",
        "visualize_gp_belief(model, likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qkkR2zkWZHsH"
      },
      "outputs": [],
      "source": [
        "class MaternGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood, nu):\n",
        "        super().__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ZeroMean()\n",
        "        self.covar_module = gpytorch.kernels.MaternKernel(nu)\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Db9Gm3B3ZKGx"
      },
      "outputs": [],
      "source": [
        "# declare the GP\n",
        "lengthscale = 1\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = MaternGPModel(train_x, train_y, likelihood, 2.5)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.covar_module.lengthscale = lengthscale\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "\n",
        "visualize_gp_belief(model, likelihood)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dbzGtYlZZN2E"
      },
      "outputs": [],
      "source": [
        "# declare the GP\n",
        "lengthscale = 1\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = MaternGPModel(train_x, train_y, likelihood, 1.5)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.covar_module.lengthscale = lengthscale\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "\n",
        "visualize_gp_belief(model, likelihood)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WDj8IAcS0c5J"
      },
      "source": [
        "# Go 2D!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q4vU8PB_ZQ1c"
      },
      "outputs": [],
      "source": [
        "def ackley(x):\n",
        "    # a modification of https://www.sfu.ca/~ssurjano/ackley.html\n",
        "    return -20 * torch.exp(\n",
        "        -0.2 * torch.sqrt((x[:, 0] ** 2 + x[:, 1] ** 2) / 2)\n",
        "    ) - torch.exp(torch.cos(2 * pi * x[:, 0] / 3) + torch.cos(2 * pi * x[:, 1]))\n",
        "\n",
        "\n",
        "xs = torch.linspace(-5, 5, 101)\n",
        "x1, x2 = torch.meshgrid(xs, xs, indexing=\"ij\")\n",
        "xs = torch.vstack((x1.flatten(), x2.flatten())).transpose(-1, -2)\n",
        "ys = ackley(xs)\n",
        "\n",
        "plt.figure(figsize=(8, 8))\n",
        "\n",
        "plt.imshow(ys.reshape(101, 101).T, origin=\"lower\", extent=[-3, 3, -3, 3])\n",
        "plt.colorbar();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yx8U3SUqZTYR"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(0)\n",
        "train_x = torch.rand(size=(100, 2)) * 6 - 3\n",
        "train_y = ackley(train_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MyIeXqjkZWXq"
      },
      "outputs": [],
      "source": [
        "class ARDGPModel(gpytorch.models.ExactGP):\n",
        "    def __init__(self, train_x, train_y, likelihood):\n",
        "        super().__init__(train_x, train_y, likelihood)\n",
        "        self.mean_module = gpytorch.means.ZeroMean()\n",
        "        self.covar_module = gpytorch.kernels.ScaleKernel(\n",
        "            gpytorch.kernels.RBFKernel(ard_num_dims=2)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        mean_x = self.mean_module(x)\n",
        "        covar_x = self.covar_module(x)\n",
        "        return gpytorch.distributions.MultivariateNormal(mean_x, covar_x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Xx0iD0AZYsU"
      },
      "outputs": [],
      "source": [
        "# declare the GP\n",
        "noise = 1e-4\n",
        "\n",
        "likelihood = gpytorch.likelihoods.GaussianLikelihood()\n",
        "model = ARDGPModel(train_x, train_y, likelihood)\n",
        "\n",
        "# fix the hyperparameters\n",
        "model.likelihood.noise = noise\n",
        "\n",
        "# train the hyperparameter (the constant)\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
        "mll = gpytorch.mlls.ExactMarginalLogLikelihood(likelihood, model)\n",
        "\n",
        "model.train()\n",
        "likelihood.train()\n",
        "\n",
        "losses = []\n",
        "x_lengthscales = []\n",
        "y_lengthscales = []\n",
        "outputscales = []\n",
        "for i in tqdm(range(500)):\n",
        "    optimizer.zero_grad()\n",
        "\n",
        "    output = model(train_x)\n",
        "    loss = -mll(output, train_y)\n",
        "\n",
        "    loss.backward()\n",
        "\n",
        "    losses.append(loss.item())\n",
        "    x_lengthscales.append(model.covar_module.base_kernel.lengthscale[0, 0].item())\n",
        "    y_lengthscales.append(model.covar_module.base_kernel.lengthscale[0, 1].item())\n",
        "    outputscales.append(model.covar_module.outputscale.item())\n",
        "\n",
        "    optimizer.step()\n",
        "\n",
        "model.eval()\n",
        "likelihood.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lo_CyyM5ZbWZ"
      },
      "outputs": [],
      "source": [
        "fig, ax = plt.subplots(1, 3, figsize=(12, 4))\n",
        "\n",
        "ax[0].plot(losses)\n",
        "ax[0].set_ylabel(\"negative marginal log likelihood\")\n",
        "\n",
        "ax[1].plot(x_lengthscales)\n",
        "ax[1].set_ylabel(r\"$x$-axis lengthscale\")\n",
        "\n",
        "ax[2].plot(y_lengthscales)\n",
        "ax[2].set_ylabel(r\"$y$-axis lengthscale\")\n",
        "\n",
        "plt.tight_layout();"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SlEIdT0MZePn"
      },
      "outputs": [],
      "source": [
        "model.covar_module.base_kernel.lengthscale"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6rDto6Qu0mvL"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from math import pi\n",
        "\n",
        "'''\n",
        "# Ackley function definition\n",
        "def ackley(x):\n",
        "    return -20 * torch.exp(-0.2 * torch.sqrt((x[:, 0] ** 2 + x[:, 1] ** 2) / 2)) - \\\n",
        "           torch.exp(torch.cos(2 * pi * x[:, 0] / 3) + torch.cos(2 * pi * x[:, 1]))\n",
        "'''\n",
        "\n",
        "# Create input grid\n",
        "xs = torch.linspace(-5, 5, 101)\n",
        "x1, x2 = torch.meshgrid(xs, xs, indexing=\"ij\")\n",
        "grid = torch.stack([x1.reshape(-1), x2.reshape(-1)], dim=-1)\n",
        "\n",
        "# Evaluate Ackley function\n",
        "ys = ackley(grid)\n",
        "\n",
        "# GP predictions\n",
        "model.eval()\n",
        "likelihood.eval()\n",
        "with torch.no_grad():\n",
        "    preds = likelihood(model(grid))\n",
        "    mean = preds.mean.reshape(101, 101).numpy()\n",
        "    std = preds.stddev.reshape(101, 101).numpy()\n",
        "\n",
        "# Plot side-by-side\n",
        "fig, axs = plt.subplots(1, 3, figsize=(18, 5))\n",
        "\n",
        "# True function\n",
        "axs[0].imshow(ys.reshape(101, 101).T, extent=[-5, 5, -5, 5], origin=\"lower\", cmap=\"viridis\")\n",
        "axs[0].set_title(\"True Ackley Function\")\n",
        "axs[0].set_xlabel(\"x₁\")\n",
        "axs[0].set_ylabel(\"x₂\")\n",
        "fig.colorbar(axs[0].images[0], ax=axs[0])\n",
        "\n",
        "# GP mean\n",
        "axs[1].imshow(mean.T, extent=[-5, 5, -5, 5], origin=\"lower\", cmap=\"viridis\")\n",
        "axs[1].set_title(\"GP Mean Prediction\")\n",
        "axs[1].set_xlabel(\"x₁\")\n",
        "axs[1].set_ylabel(\"x₂\")\n",
        "fig.colorbar(axs[1].images[0], ax=axs[1])\n",
        "\n",
        "# GP uncertainty\n",
        "axs[2].imshow(std.T, extent=[-5, 5, -5, 5], origin=\"lower\", cmap=\"inferno\")\n",
        "axs[2].set_title(\"GP Predictive Std Dev (Uncertainty)\")\n",
        "axs[2].set_xlabel(\"x₁\")\n",
        "axs[2].set_ylabel(\"x₂\")\n",
        "fig.colorbar(axs[2].images[0], ax=axs[2])\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: generate a 2x2 plot of functions x^2, 1/x. sin(x), and tanh(x)\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# Define the functions\n",
        "def f1(x):\n",
        "  return x**2\n",
        "\n",
        "def f2(x):\n",
        "  return 1/x\n",
        "\n",
        "def f3(x):\n",
        "  return np.sin(x)\n",
        "\n",
        "def f4(x):\n",
        "  return np.tanh(x)\n",
        "\n",
        "# Create the x-values\n",
        "x = np.linspace(-5, 5, 100)\n",
        "\n",
        "# Create the subplots\n",
        "fig, axes = plt.subplots(2, 2, figsize=(10, 8))\n",
        "\n",
        "# Plot the functions\n",
        "axes[0, 0].plot(x, f1(x))\n",
        "axes[0, 0].set_title('x^2')\n",
        "\n",
        "axes[0, 1].plot(x, f2(x))\n",
        "axes[0, 1].set_title('1/x')\n",
        "\n",
        "axes[1, 0].plot(x, f3(x))\n",
        "axes[1, 0].set_title('sin(x)')\n",
        "\n",
        "axes[1, 1].plot(x, f4(x))\n",
        "axes[1, 1].set_title('tanh(x)')\n",
        "\n",
        "# Adjust layout and display\n",
        "plt.tight_layout()\n",
        "plt.show()\n"
      ],
      "metadata": {
        "id": "_JGjkbrDjUnu",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNaLfYQg8/mTBoammJrVykN",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}