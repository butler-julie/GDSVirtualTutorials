{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://colab.research.google.com/github/butler-julie/GDSVirtualTutorials/blob/main/060625_UncertaintyQuantification/uncertainty_quantification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n",
    "\n",
    "# Uncertainty Quantification Tutorial\n",
    "\n",
    "Ashley S. Dale"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook introduces a simple method of estimating *aleatoric* and *epistemic* uncertainty for random forest regression (RFR) models based on the conservation of variance.\n",
    "\n",
    "**Epistemic Uncertainty** can be reduced through further information available to the model.\n",
    "\n",
    "**Aleatoric Uncertainty** cannot be reduced; it is part of the randomness associated with model hyperparameters.\n",
    "\n",
    "**Total Uncertainty** = Epistemic Uncertainty + Aleatoric Uncertainty\n",
    "\n",
    "We would like to distinguish between the two so that we can better understand how to select data samples for tasks.  However, estimating the aleatoric uncertainty is challenging.  It is more common to calculate the total uncertainty and epistemic uncertainty, then solve for the aleatoric uncertainty.  \n",
    "\n",
    "$aleatoric\\ uncertainty = (total\\ uncertainty) - (epistemic\\ uncertainty)$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Procedure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Given a dataset, divide the data into a training and a test set\n",
    "    - The train set consists of chemistries which do not contain Fe as an element\n",
    "    - The test set consists of chemistries which *do* contain Fe as an element\n",
    "2. Train a Random Forest Regression model to predict the energy of formation for these chemistries\n",
    "3. Estimate the total uncertainty of the predictions\n",
    "4. Estimate the epistemic uncertainty of the predictions\n",
    "5. Calculate an estimate of the aleatoric uncertainty for the predictions\n",
    "6. Identify samples which have low aleatoric and high epistemic uncertainty"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "import numpy as np\n",
    "import datetime\n",
    "import pandas as pd\n",
    "from sklearn.metrics import root_mean_squared_error as rmse\n",
    "from sklearn.metrics import r2_score as r2\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm, trange\n",
    "\n",
    "from data_utils import get_samples_w_element_X, get_target_label\n",
    "\n",
    "from calibration import calculate_density, calculate_miscalibration_area, calculate_calibration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data\n",
    "\n",
    "We will use a version of the Jarvis3D DFT dataset. The total file size is 211 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#download the data pickle\n",
    "!chmod 755 get_featurized_data.bash\n",
    "!./get_featurized_data.bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get Data\n",
    "data = pd.read_pickle('data/jarvis22/dat_featurized_matminer.pkl')\n",
    "print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = 'e_form'\n",
    "n_samples = -1 # for all samples, pass -1\n",
    "element_to_omit_from_training_data = 'Fe'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = get_samples_w_element_X(data, 'formula', element_to_omit_from_training_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test, y_test = get_target_label(test_data, target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, y_train = get_target_label(train_data, target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Random Forest Regression Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_trees_in_forest = 100\n",
    "max_feat = 0.1\n",
    "num_dataset_features = X_test.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RandomForestRegressor(\n",
    "    n_estimators=num_trees_in_forest, \n",
    "    max_features=max_feat,\n",
    "    oob_score=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_train = model.predict(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_test= model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_predictions = []\n",
    "train_predictions = []\n",
    "\n",
    "# Get the predictions from each tree in the forest\n",
    "for tree_obj in tqdm(model.estimators_, total = model.n_estimators):\n",
    "    test_predictions.append(tree_obj.tree_.predict(X_test.astype(np.float32)))\n",
    "    train_predictions.append(tree_obj.tree_.predict(X_train.astype(np.float32)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_train_predictions = np.transpose(np.squeeze(np.array(train_predictions)))\n",
    "mean_of_ea_train_pred = np.mean(set_of_train_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_of_test_predictions = np.transpose(np.squeeze(np.array(test_predictions)))\n",
    "mean_of_ea_test_pred = np.mean(set_of_test_predictions, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confirm Model Accuracy\n",
    "\n",
    "To check the model performance, we will make a parity plot of the predictions vs ground truth."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].scatter(y_train, y_pred_train, label=f'R2: {r2(y_train, y_pred_train): .2f}')\n",
    "ax[0].set_title('Training Data Predictions')\n",
    "ax[1].scatter(y_test, y_pred_test, label=f'R2: {r2(y_test, y_pred_test): .2f}')\n",
    "ax[1].set_title('Test Data Predictions')\n",
    "\n",
    "for ax_ in ax:\n",
    "    ax_.set_xlabel('True Values')\n",
    "    ax_.set_ylabel('Predicted Values')\n",
    "    ax_.plot([ax_.get_xlim()[0], ax_.get_xlim()[1]], [ax_.get_xlim()[0], ax_.get_xlim()[1]], 'k--', lw=2)\n",
    "    ax_.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Total Uncertainty for Each Sample\n",
    "\n",
    "The total uncertainty for each prediction from a Random Forest Regression model is commonly estimated using the Mean Squared Error of the prediction. For each prediction $y_i$ from the $i^\\text{th}$ tree out of $n$ trees in the Random Forest and the target value $y$:\n",
    "\n",
    "$\\sigma^2 = \\frac{1}{n} \\Sigma_n \\left( y - y_i\\right )^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_estimate(y, y_hat, n=None):\n",
    "    if n == None:\n",
    "        n = len(y_hat)\n",
    "    return np.sum(np.power((y - y_hat), 2))/n\n",
    "\n",
    "def get_variance_estimate(true_values, predicted_values):\n",
    "    var = []\n",
    "    for sample_idx in trange(len(true_values)):\n",
    "        var.append(variance_estimate(true_values[sample_idx],predicted_values[sample_idx, :] ))\n",
    "    return np.array(var)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_total_var = get_variance_estimate(y_train, set_of_train_predictions)\n",
    "test_total_var = get_variance_estimate(y_test, set_of_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate Epistemic Uncertainty\n",
    "\n",
    "The epistemic uncertainty for a Random Forest Regression model is approximated using the variance in predictions across all trees. Given a mean predicted value $\\bar{y}$ from the set of predictions $y_i$ from the $i^\\text{th}$ \n",
    "\n",
    "$\\bar{y} = \\frac{1}{n} \\Sigma_n \\left(y_i\\right)^2$\n",
    "\n",
    "The variance in predictions between all trees in the forest is\n",
    "\n",
    "$\\sigma_\\text{epi}^2 = \\frac{1}{n} \\Sigma_n (y_i - \\bar{y})^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_explained_var = get_variance_estimate(mean_of_ea_train_pred, set_of_train_predictions)\n",
    "test_explained_var = get_variance_estimate(mean_of_ea_test_pred, set_of_test_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Aleatoric Uncertainty\n",
    "\n",
    "To obtain an estimate of the aleatoric uncertainty $\\sigma_\\text{al}^2$, we obtain the difference between the total uncertainty and the prediction variance:\n",
    "\n",
    "$\\sigma_\\text{al}^2 = \\sigma^2 - \\sigma_\\text{epi}^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_diff_var = train_total_var - train_explained_var\n",
    "test_diff_var = test_total_var - test_explained_var"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Model Calibration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "residuals_train = np.array(y_train) - np.mean(set_of_train_predictions)\n",
    "stddev_train = np.std(set_of_train_predictions)\n",
    "\n",
    "residuals_test = np.array(y_test) - np.mean(set_of_test_predictions)\n",
    "stddev_test = np.std(set_of_test_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_pi = np.linspace(0, 1, 100)\n",
    "obsv_pi_train = calculate_calibration(residuals_train, stddev_train)\n",
    "obsv_pi_test = calculate_calibration(residuals_test, stddev_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cal_err_id = ((predicted_pi - obsv_pi_train)**2).sum()\n",
    "cal_area_id = calculate_miscalibration_area(predicted_pi, obsv_pi_train)\n",
    "\n",
    "cal_err_ood = ((predicted_pi - obsv_pi_test)**2).sum()\n",
    "cal_area_ood = calculate_miscalibration_area(predicted_pi, obsv_pi_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "ax[0].fill_between(\n",
    "    predicted_pi, predicted_pi, obsv_pi_train, \n",
    "    label=f'Train Data: Cal Err={cal_err_id:.3f}, AUC={cal_area_id:.3f}', alpha=0.51)\n",
    "ax[0].plot(predicted_pi, predicted_pi, '--', color='k', alpha=0.5)\n",
    "ax[0].set_xlabel('Expected Frequency')\n",
    "ax[0].set_ylabel('Observed Frequency')\n",
    "ax[0].set_title(f'{target} {element_to_omit_from_training_data}')\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].fill_between(\n",
    "    predicted_pi, predicted_pi, obsv_pi_test, alpha=0.51, \n",
    "    label=f'Test Data: Cal Err={cal_err_ood:.3f}, AUC={cal_area_ood:.3f}', color='tab:orange')\n",
    "ax[1].plot(predicted_pi, predicted_pi, '--', color='k', alpha=0.5)\n",
    "ax[1].set_xlabel('Expected Frequency')\n",
    "ax[1].set_ylabel('Observed Frequency')\n",
    "ax[1].set_title(f'{target} {element_to_omit_from_training_data}')\n",
    "ax[1].legend()\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Compare Aleatoric and Epistemic Uncertainties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we visualize the total uncertainty estimate for each prediction by creating a parity plot with error bars.  To preserve units, the error bars are presented using the standard devation of the total uncertainty for each sample:\n",
    "\n",
    "$\\sigma = \\sqrt{\\frac{1}{n}\\Sigma_n \\left(y - y_i \\right)^2}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(9, 4))\n",
    "ax[0].errorbar(\n",
    "    y_train, mean_of_ea_train_pred, yerr=np.sqrt(train_total_var), \n",
    "    fmt='.', linestyle=None, alpha=0.5, label='R2: ' +str(r2(y_train, mean_of_ea_train_pred))[:6]\n",
    "    )\n",
    "ax[0].plot(y_train, y_train, 'r')\n",
    "ax[0].set_title('Training')\n",
    "ax[0].set_xlabel('Target')\n",
    "ax[0].set_ylabel('Mean Prediction')\n",
    "ax[0].set_xlim(1.1*np.amin(y_train), 1.1*np.amax(y_train))\n",
    "ax[0].set_ylim(1.1*np.amin(y_train), 1.1*np.amax(y_train))\n",
    "ax[0].legend()\n",
    "\n",
    "ax[1].errorbar(\n",
    "    y_test, mean_of_ea_test_pred, yerr=np.sqrt(test_total_var), \n",
    "    fmt='.', linestyle=None, alpha=0.5, color='C1',  label='R2: ' +str(r2(y_test, mean_of_ea_test_pred))[:6]\n",
    "    )\n",
    "\n",
    "ax[1].plot(y_test, y_test, 'r')\n",
    "ax[1].set_title('Testing')\n",
    "ax[1].set_xlabel('Target')\n",
    "ax[1].set_ylabel('Mean Prediction')\n",
    "ax[1].set_xlim(1.1*np.amin(y_test), 1.1*np.amax(y_test))\n",
    "ax[1].set_ylim(1.1*np.amin(y_test), 1.1*np.amax(y_test))\n",
    "ax[1].legend()\n",
    "\n",
    "fig.suptitle(f'Random Forest Predictions: Train omits {element_to_omit_from_training_data}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also visualize the distributions of the standard deviation:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1, figsize=(5, 5))\n",
    "_ = ax.hist(np.sqrt(train_total_var), 100, density=True, alpha=0.7, label='Train')\n",
    "_ = ax.hist(np.sqrt(test_total_var), 100, density=True, alpha=0.7, label='Test')\n",
    "ax.set_yticks(())\n",
    "ax.set_xlabel('Sample Uncertainties')\n",
    "ax.set_ylabel('Frequency')\n",
    "ax.set_title('Total Uncertainty')\n",
    "ax.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we can can check for correlation between epistemic and aleatoric uncertainty:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 2, figsize=(8, 4))\n",
    "\n",
    "ax[0].scatter(train_explained_var, train_diff_var, alpha=0.5, color='C0', label='Train', s=8)\n",
    "ax[0].set_xlabel('Epistemic Uncertainty')\n",
    "ax[0].set_ylabel('Aleatoric Uncertainty')\n",
    "ax[0].set_title('Train')\n",
    "ax[0].set_yscale('log')\n",
    "ax[0].set_xscale('log')\n",
    "\n",
    "ax[1].scatter(test_explained_var, test_diff_var, alpha=0.5, color='C1', label='Test', s=10)\n",
    "ax[1].set_xlabel('Epistemic Uncertainty')\n",
    "ax[1].set_ylabel('Aleatoric Uncertainty')\n",
    "ax[1].set_title('Test')\n",
    "ax[1].set_yscale('log')\n",
    "ax[1].set_xscale('log')\n",
    "fig.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
